"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class GoogleCloudDatalabelingV1beta1EventConfig:
    r"""Config for video event human labeling task."""
    
    annotation_spec_sets: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('annotationSpecSets'), 'exclude': lambda f: f is None }})
    r"""Required. The list of annotation spec set resource name. Similar to video classification, we support selecting event from multiple AnnotationSpecSet at the same time."""  
    clip_length: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('clipLength'), 'exclude': lambda f: f is None }})
    r"""Videos will be cut to smaller clips to make it easier for labelers to work on. Users can configure is field in seconds, if not set, default value is 60s."""  
    overlap_length: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('overlapLength'), 'exclude': lambda f: f is None }})
    r"""The overlap length between different video clips. Users can configure is field in seconds, if not set, default value is 1s."""  
    