"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import parquetcompression_enum as shared_parquetcompression_enum
from ..shared import parquetwriterversion_enum as shared_parquetwriterversion_enum
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ParquetSerDe:
    r"""A serializer to use for converting data to the Parquet format before storing it in Amazon S3. For more information, see <a href=\\"https://parquet.apache.org/documentation/latest/\\">Apache Parquet</a>."""
    
    block_size_bytes: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('BlockSizeBytes'), 'exclude': lambda f: f is None }})  
    compression: Optional[shared_parquetcompression_enum.ParquetCompressionEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Compression'), 'exclude': lambda f: f is None }})  
    enable_dictionary_compression: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('EnableDictionaryCompression'), 'exclude': lambda f: f is None }})  
    max_padding_bytes: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MaxPaddingBytes'), 'exclude': lambda f: f is None }})  
    page_size_bytes: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('PageSizeBytes'), 'exclude': lambda f: f is None }})  
    writer_version: Optional[shared_parquetwriterversion_enum.ParquetWriterVersionEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('WriterVersion'), 'exclude': lambda f: f is None }})  
    