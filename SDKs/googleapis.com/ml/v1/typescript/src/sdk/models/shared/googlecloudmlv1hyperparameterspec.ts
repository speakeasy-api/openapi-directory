/*
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

import { SpeakeasyBase, SpeakeasyMetadata } from "../../../internal/utils";
import { GoogleCloudMlV1ParameterSpec } from "./googlecloudmlv1parameterspec";
import { Expose, Type } from "class-transformer";

/**
 * Optional. The search algorithm specified for the hyperparameter tuning job. Uses the default AI Platform hyperparameter tuning algorithm if unspecified.
 */
export enum GoogleCloudMlV1HyperparameterSpecAlgorithmEnum {
  AlgorithmUnspecified = "ALGORITHM_UNSPECIFIED",
  GridSearch = "GRID_SEARCH",
  RandomSearch = "RANDOM_SEARCH",
}

/**
 * Required. The type of goal to use for tuning. Available types are `MAXIMIZE` and `MINIMIZE`. Defaults to `MAXIMIZE`.
 */
export enum GoogleCloudMlV1HyperparameterSpecGoalEnum {
  GoalTypeUnspecified = "GOAL_TYPE_UNSPECIFIED",
  Maximize = "MAXIMIZE",
  Minimize = "MINIMIZE",
}

/**
 * Represents a set of hyperparameters to optimize.
 */
export class GoogleCloudMlV1HyperparameterSpec extends SpeakeasyBase {
  /**
   * Optional. The search algorithm specified for the hyperparameter tuning job. Uses the default AI Platform hyperparameter tuning algorithm if unspecified.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "algorithm" })
  algorithm?: GoogleCloudMlV1HyperparameterSpecAlgorithmEnum;

  /**
   * Optional. Indicates if the hyperparameter tuning job enables auto trial early stopping.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "enableTrialEarlyStopping" })
  enableTrialEarlyStopping?: boolean;

  /**
   * Required. The type of goal to use for tuning. Available types are `MAXIMIZE` and `MINIMIZE`. Defaults to `MAXIMIZE`.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "goal" })
  goal?: GoogleCloudMlV1HyperparameterSpecGoalEnum;

  /**
   * Optional. The TensorFlow summary tag name to use for optimizing trials. For current versions of TensorFlow, this tag name should exactly match what is shown in TensorBoard, including all scopes. For versions of TensorFlow prior to 0.12, this should be only the tag passed to tf.Summary. By default, "training/hptuning/metric" will be used.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "hyperparameterMetricTag" })
  hyperparameterMetricTag?: string;

  /**
   * Optional. The number of failed trials that need to be seen before failing the hyperparameter tuning job. You can specify this field to override the default failing criteria for AI Platform hyperparameter tuning jobs. Defaults to zero, which means the service decides when a hyperparameter job should fail.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "maxFailedTrials" })
  maxFailedTrials?: number;

  /**
   * Optional. The number of training trials to run concurrently. You can reduce the time it takes to perform hyperparameter tuning by adding trials in parallel. However, each trail only benefits from the information gained in completed trials. That means that a trial does not get access to the results of trials running at the same time, which could reduce the quality of the overall optimization. Each trial will use the same scale tier and machine types. Defaults to one.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "maxParallelTrials" })
  maxParallelTrials?: number;

  /**
   * Optional. How many training trials should be attempted to optimize the specified hyperparameters. Defaults to one.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "maxTrials" })
  maxTrials?: number;

  /**
   * Required. The set of parameters to tune.
   */
  @SpeakeasyMetadata({ elemType: GoogleCloudMlV1ParameterSpec })
  @Expose({ name: "params" })
  @Type(() => GoogleCloudMlV1ParameterSpec)
  params?: GoogleCloudMlV1ParameterSpec[];

  /**
   * Optional. The prior hyperparameter tuning job id that users hope to continue with. The job id will be used to find the corresponding vizier study guid and resume the study.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "resumePreviousJobId" })
  resumePreviousJobId?: string;
}
