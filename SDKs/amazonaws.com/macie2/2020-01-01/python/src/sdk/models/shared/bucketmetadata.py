"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import dateutil.parser
from ..shared import allowsunencryptedobjectuploads_enum as shared_allowsunencryptedobjectuploads_enum
from ..shared import bucketmetadataerrorcode_enum as shared_bucketmetadataerrorcode_enum
from ..shared import bucketpublicaccess as shared_bucketpublicaccess
from ..shared import bucketserversideencryption as shared_bucketserversideencryption
from ..shared import jobdetails as shared_jobdetails
from ..shared import keyvaluepair as shared_keyvaluepair
from ..shared import objectcountbyencryptiontype as shared_objectcountbyencryptiontype
from ..shared import objectlevelstatistics as shared_objectlevelstatistics
from ..shared import replicationdetails as shared_replicationdetails
from ..shared import sharedaccess_enum as shared_sharedaccess_enum
from dataclasses_json import Undefined, dataclass_json
from datetime import datetime
from marshmallow import fields
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketMetadata:
    r"""<p>Provides statistical data and other information about an S3 bucket that Amazon Macie monitors and analyzes for your account. By default, object count and storage size values include data for object parts that are the result of incomplete multipart uploads. For more information, see <a href=\\"https://docs.aws.amazon.com/macie/latest/user/monitoring-s3-how-it-works.html\\">How Macie monitors Amazon S3 data security</a> in the <i>Amazon Macie User Guide</i>.</p> <p>If an error occurs when Macie attempts to retrieve and process metadata from Amazon S3 for the bucket or the bucket's objects, the value for the versioning property is false and the value for most other properties is null. Key exceptions are accountId, bucketArn, bucketCreatedAt, bucketName, lastUpdated, and region. To identify the cause of the error, refer to the errorCode and errorMessage values.</p>"""
    
    account_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('accountId'), 'exclude': lambda f: f is None }})  
    allows_unencrypted_object_uploads: Optional[shared_allowsunencryptedobjectuploads_enum.AllowsUnencryptedObjectUploadsEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('allowsUnencryptedObjectUploads'), 'exclude': lambda f: f is None }})  
    bucket_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucketArn'), 'exclude': lambda f: f is None }})  
    bucket_created_at: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucketCreatedAt'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})  
    bucket_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucketName'), 'exclude': lambda f: f is None }})  
    classifiable_object_count: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('classifiableObjectCount'), 'exclude': lambda f: f is None }})  
    classifiable_size_in_bytes: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('classifiableSizeInBytes'), 'exclude': lambda f: f is None }})  
    error_code: Optional[shared_bucketmetadataerrorcode_enum.BucketMetadataErrorCodeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('errorCode'), 'exclude': lambda f: f is None }})  
    error_message: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('errorMessage'), 'exclude': lambda f: f is None }})  
    job_details: Optional[shared_jobdetails.JobDetails] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobDetails'), 'exclude': lambda f: f is None }})  
    last_automated_discovery_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lastAutomatedDiscoveryTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})  
    last_updated: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lastUpdated'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})  
    object_count: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('objectCount'), 'exclude': lambda f: f is None }})  
    object_count_by_encryption_type: Optional[shared_objectcountbyencryptiontype.ObjectCountByEncryptionType] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('objectCountByEncryptionType'), 'exclude': lambda f: f is None }})  
    public_access: Optional[shared_bucketpublicaccess.BucketPublicAccess] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('publicAccess'), 'exclude': lambda f: f is None }})  
    region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('region'), 'exclude': lambda f: f is None }})  
    replication_details: Optional[shared_replicationdetails.ReplicationDetails] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('replicationDetails'), 'exclude': lambda f: f is None }})  
    sensitivity_score: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sensitivityScore'), 'exclude': lambda f: f is None }})  
    server_side_encryption: Optional[shared_bucketserversideencryption.BucketServerSideEncryption] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serverSideEncryption'), 'exclude': lambda f: f is None }})  
    shared_access: Optional[shared_sharedaccess_enum.SharedAccessEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sharedAccess'), 'exclude': lambda f: f is None }})  
    size_in_bytes: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sizeInBytes'), 'exclude': lambda f: f is None }})  
    size_in_bytes_compressed: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sizeInBytesCompressed'), 'exclude': lambda f: f is None }})  
    tags: Optional[list[shared_keyvaluepair.KeyValuePair]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tags'), 'exclude': lambda f: f is None }})  
    unclassifiable_object_count: Optional[shared_objectlevelstatistics.ObjectLevelStatistics] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('unclassifiableObjectCount'), 'exclude': lambda f: f is None }})  
    unclassifiable_object_size_in_bytes: Optional[shared_objectlevelstatistics.ObjectLevelStatistics] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('unclassifiableObjectSizeInBytes'), 'exclude': lambda f: f is None }})  
    versioning: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('versioning'), 'exclude': lambda f: f is None }})  
    