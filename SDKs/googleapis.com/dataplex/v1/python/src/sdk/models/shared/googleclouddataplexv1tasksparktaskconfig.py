"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import googleclouddataplexv1taskinfrastructurespec as shared_googleclouddataplexv1taskinfrastructurespec
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class GoogleCloudDataplexV1TaskSparkTaskConfig:
    r"""User-specified config for running a Spark task."""
    
    archive_uris: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('archiveUris'), 'exclude': lambda f: f is None }})
    r"""Optional. Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip."""  
    file_uris: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('fileUris'), 'exclude': lambda f: f is None }})
    r"""Optional. Cloud Storage URIs of files to be placed in the working directory of each executor."""  
    infrastructure_spec: Optional[shared_googleclouddataplexv1taskinfrastructurespec.GoogleCloudDataplexV1TaskInfrastructureSpec] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('infrastructureSpec'), 'exclude': lambda f: f is None }})
    r"""Configuration for the underlying infrastructure used to run workloads."""  
    main_class: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('mainClass'), 'exclude': lambda f: f is None }})
    r"""The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris. The execution args are passed in as a sequence of named process arguments (--key=value)."""  
    main_jar_file_uri: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('mainJarFileUri'), 'exclude': lambda f: f is None }})
    r"""The Cloud Storage URI of the jar file that contains the main class. The execution args are passed in as a sequence of named process arguments (--key=value)."""  
    python_script_file: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pythonScriptFile'), 'exclude': lambda f: f is None }})
    r"""The Gcloud Storage URI of the main Python file to use as the driver. Must be a .py file. The execution args are passed in as a sequence of named process arguments (--key=value)."""  
    sql_script: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sqlScript'), 'exclude': lambda f: f is None }})
    r"""The query text. The execution args are used to declare a set of script variables (set key=\\"value\\";)."""  
    sql_script_file: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sqlScriptFile'), 'exclude': lambda f: f is None }})
    r"""A reference to a query file. This can be the Cloud Storage URI of the query file or it can the path to a SqlScript Content. The execution args are used to declare a set of script variables (set key=\\"value\\";)."""  
    