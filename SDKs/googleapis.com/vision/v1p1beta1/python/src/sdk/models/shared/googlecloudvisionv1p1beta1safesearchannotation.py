"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional

class GoogleCloudVisionV1p1beta1SafeSearchAnnotationAdultEnum(str, Enum):
    r"""Represents the adult content likelihood for the image. Adult content may contain elements such as nudity, pornographic images or cartoons, or sexual activities."""
    UNKNOWN = 'UNKNOWN'
    VERY_UNLIKELY = 'VERY_UNLIKELY'
    UNLIKELY = 'UNLIKELY'
    POSSIBLE = 'POSSIBLE'
    LIKELY = 'LIKELY'
    VERY_LIKELY = 'VERY_LIKELY'

class GoogleCloudVisionV1p1beta1SafeSearchAnnotationMedicalEnum(str, Enum):
    r"""Likelihood that this is a medical image."""
    UNKNOWN = 'UNKNOWN'
    VERY_UNLIKELY = 'VERY_UNLIKELY'
    UNLIKELY = 'UNLIKELY'
    POSSIBLE = 'POSSIBLE'
    LIKELY = 'LIKELY'
    VERY_LIKELY = 'VERY_LIKELY'

class GoogleCloudVisionV1p1beta1SafeSearchAnnotationRacyEnum(str, Enum):
    r"""Likelihood that the request image contains racy content. Racy content may include (but is not limited to) skimpy or sheer clothing, strategically covered nudity, lewd or provocative poses, or close-ups of sensitive body areas."""
    UNKNOWN = 'UNKNOWN'
    VERY_UNLIKELY = 'VERY_UNLIKELY'
    UNLIKELY = 'UNLIKELY'
    POSSIBLE = 'POSSIBLE'
    LIKELY = 'LIKELY'
    VERY_LIKELY = 'VERY_LIKELY'

class GoogleCloudVisionV1p1beta1SafeSearchAnnotationSpoofEnum(str, Enum):
    r"""Spoof likelihood. The likelihood that an modification was made to the image's canonical version to make it appear funny or offensive."""
    UNKNOWN = 'UNKNOWN'
    VERY_UNLIKELY = 'VERY_UNLIKELY'
    UNLIKELY = 'UNLIKELY'
    POSSIBLE = 'POSSIBLE'
    LIKELY = 'LIKELY'
    VERY_LIKELY = 'VERY_LIKELY'

class GoogleCloudVisionV1p1beta1SafeSearchAnnotationViolenceEnum(str, Enum):
    r"""Likelihood that this image contains violent content."""
    UNKNOWN = 'UNKNOWN'
    VERY_UNLIKELY = 'VERY_UNLIKELY'
    UNLIKELY = 'UNLIKELY'
    POSSIBLE = 'POSSIBLE'
    LIKELY = 'LIKELY'
    VERY_LIKELY = 'VERY_LIKELY'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class GoogleCloudVisionV1p1beta1SafeSearchAnnotation:
    r"""Set of features pertaining to the image, computed by computer vision methods over safe-search verticals (for example, adult, spoof, medical, violence)."""
    
    adult: Optional[GoogleCloudVisionV1p1beta1SafeSearchAnnotationAdultEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('adult'), 'exclude': lambda f: f is None }})
    r"""Represents the adult content likelihood for the image. Adult content may contain elements such as nudity, pornographic images or cartoons, or sexual activities."""  
    medical: Optional[GoogleCloudVisionV1p1beta1SafeSearchAnnotationMedicalEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('medical'), 'exclude': lambda f: f is None }})
    r"""Likelihood that this is a medical image."""  
    racy: Optional[GoogleCloudVisionV1p1beta1SafeSearchAnnotationRacyEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('racy'), 'exclude': lambda f: f is None }})
    r"""Likelihood that the request image contains racy content. Racy content may include (but is not limited to) skimpy or sheer clothing, strategically covered nudity, lewd or provocative poses, or close-ups of sensitive body areas."""  
    spoof: Optional[GoogleCloudVisionV1p1beta1SafeSearchAnnotationSpoofEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('spoof'), 'exclude': lambda f: f is None }})
    r"""Spoof likelihood. The likelihood that an modification was made to the image's canonical version to make it appear funny or offensive."""  
    violence: Optional[GoogleCloudVisionV1p1beta1SafeSearchAnnotationViolenceEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('violence'), 'exclude': lambda f: f is None }})
    r"""Likelihood that this image contains violent content."""  
    