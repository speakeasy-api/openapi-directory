"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import hadoopjob as shared_hadoopjob
from ..shared import hivejob as shared_hivejob
from ..shared import jobscheduling as shared_jobscheduling
from ..shared import pigjob as shared_pigjob
from ..shared import prestojob as shared_prestojob
from ..shared import pysparkjob as shared_pysparkjob
from ..shared import sparkjob as shared_sparkjob
from ..shared import sparkrjob as shared_sparkrjob
from ..shared import sparksqljob as shared_sparksqljob
from ..shared import trinojob as shared_trinojob
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class OrderedJob:
    r"""A job executed by the workflow."""
    
    hadoop_job: Optional[shared_hadoopjob.HadoopJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('hadoopJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html)."""  
    hive_job: Optional[shared_hivejob.HiveJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('hiveJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN."""  
    labels: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('labels'), 'exclude': lambda f: f is None }})
    r"""Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job."""  
    pig_job: Optional[shared_pigjob.PigJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pigJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN."""  
    prerequisite_step_ids: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('prerequisiteStepIds'), 'exclude': lambda f: f is None }})
    r"""Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow."""  
    presto_job: Optional[shared_prestojob.PrestoJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('prestoJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster."""  
    pyspark_job: Optional[shared_pysparkjob.PySparkJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pysparkJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN."""  
    scheduling: Optional[shared_jobscheduling.JobScheduling] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('scheduling'), 'exclude': lambda f: f is None }})
    r"""Job scheduling options."""  
    spark_job: Optional[shared_sparkjob.SparkJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN."""  
    spark_r_job: Optional[shared_sparkrjob.SparkRJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkRJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN."""  
    spark_sql_job: Optional[shared_sparksqljob.SparkSQLJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkSqlJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries."""  
    step_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('stepId'), 'exclude': lambda f: f is None }})
    r"""Required. The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters."""  
    trino_job: Optional[shared_trinojob.TrinoJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('trinoJob'), 'exclude': lambda f: f is None }})
    r"""A Dataproc job for running Trino (https://trino.io/) queries. IMPORTANT: The Dataproc Trino Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/trino) must be enabled when the cluster is created to submit a Trino job to the cluster."""  
    