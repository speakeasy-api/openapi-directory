/* 
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

package org.openapis.openapi.models.shared;

import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * SynthesizeSpeechResponse - The message returned to the client by the `SynthesizeSpeech` method.
 */
public class SynthesizeSpeechResponse {
    /**
     * Description of audio data to be synthesized.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("audioConfig")
    public AudioConfig audioConfig;
    public SynthesizeSpeechResponse withAudioConfig(AudioConfig audioConfig) {
        this.audioConfig = audioConfig;
        return this;
    }
    
    /**
     * The audio data bytes encoded as specified in the request, including the header for encodings that are wrapped in containers (e.g. MP3, OGG_OPUS). For LINEAR16 audio, we include the WAV header. Note: as with all bytes fields, protobuffers use a pure binary representation, whereas JSON representations use base64.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("audioContent")
    public String audioContent;
    public SynthesizeSpeechResponse withAudioContent(String audioContent) {
        this.audioContent = audioContent;
        return this;
    }
    
    /**
     * A link between a position in the original request input and a corresponding time in the output audio. It's only supported via `` of SSML input.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("timepoints")
    public Timepoint[] timepoints;
    public SynthesizeSpeechResponse withTimepoints(Timepoint[] timepoints) {
        this.timepoints = timepoints;
        return this;
    }
    
}
