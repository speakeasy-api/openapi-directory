"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import dateutil.parser
from ..shared import appspecification as shared_appspecification
from ..shared import experimentconfig as shared_experimentconfig
from ..shared import networkconfig as shared_networkconfig
from ..shared import processinginput as shared_processinginput
from ..shared import processingjobstatus_enum as shared_processingjobstatus_enum
from ..shared import processingoutputconfig as shared_processingoutputconfig
from ..shared import processingresources as shared_processingresources
from ..shared import processingstoppingcondition as shared_processingstoppingcondition
from dataclasses_json import Undefined, dataclass_json
from datetime import datetime
from marshmallow import fields
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DescribeProcessingJobResponse:
    r"""Success"""
    
    app_specification: shared_appspecification.AppSpecification = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AppSpecification') }})  
    creation_time: datetime = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CreationTime'), 'encoder': utils.datetimeisoformat(False), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso') }})  
    processing_job_arn: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingJobArn') }})  
    processing_job_name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingJobName') }})  
    processing_job_status: shared_processingjobstatus_enum.ProcessingJobStatusEnum = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingJobStatus') }})  
    processing_resources: shared_processingresources.ProcessingResources = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingResources') }})  
    auto_ml_job_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AutoMLJobArn'), 'exclude': lambda f: f is None }})  
    environment: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Environment'), 'exclude': lambda f: f is None }})  
    exit_message: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ExitMessage'), 'exclude': lambda f: f is None }})  
    experiment_config: Optional[shared_experimentconfig.ExperimentConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ExperimentConfig'), 'exclude': lambda f: f is None }})  
    failure_reason: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FailureReason'), 'exclude': lambda f: f is None }})  
    last_modified_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('LastModifiedTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})  
    monitoring_schedule_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MonitoringScheduleArn'), 'exclude': lambda f: f is None }})  
    network_config: Optional[shared_networkconfig.NetworkConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('NetworkConfig'), 'exclude': lambda f: f is None }})  
    processing_end_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingEndTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})  
    processing_inputs: Optional[list[shared_processinginput.ProcessingInput]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingInputs'), 'exclude': lambda f: f is None }})  
    processing_output_config: Optional[shared_processingoutputconfig.ProcessingOutputConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingOutputConfig'), 'exclude': lambda f: f is None }})  
    processing_start_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ProcessingStartTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})  
    role_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('RoleArn'), 'exclude': lambda f: f is None }})  
    stopping_condition: Optional[shared_processingstoppingcondition.ProcessingStoppingCondition] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('StoppingCondition'), 'exclude': lambda f: f is None }})  
    training_job_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('TrainingJobArn'), 'exclude': lambda f: f is None }})  
    