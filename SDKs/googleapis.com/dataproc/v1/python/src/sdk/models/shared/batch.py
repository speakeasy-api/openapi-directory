"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import environmentconfig as shared_environmentconfig
from ..shared import pysparkbatch as shared_pysparkbatch
from ..shared import runtimeconfig as shared_runtimeconfig
from ..shared import runtimeinfo as shared_runtimeinfo
from ..shared import sparkbatch as shared_sparkbatch
from ..shared import sparkrbatch as shared_sparkrbatch
from ..shared import sparksqlbatch as shared_sparksqlbatch
from ..shared import statehistory as shared_statehistory
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional

class BatchStateEnum(str, Enum):
    r"""Output only. The state of the batch."""
    STATE_UNSPECIFIED = 'STATE_UNSPECIFIED'
    PENDING = 'PENDING'
    RUNNING = 'RUNNING'
    CANCELLING = 'CANCELLING'
    CANCELLED = 'CANCELLED'
    SUCCEEDED = 'SUCCEEDED'
    FAILED = 'FAILED'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class Batch:
    r"""A representation of a batch workload in the service."""
    
    create_time: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('createTime'), 'exclude': lambda f: f is None }})
    r"""Output only. The time when the batch was created."""  
    creator: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('creator'), 'exclude': lambda f: f is None }})
    r"""Output only. The email address of the user who created the batch."""  
    environment_config: Optional[shared_environmentconfig.EnvironmentConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('environmentConfig'), 'exclude': lambda f: f is None }})
    r"""Environment configuration for a workload."""  
    labels: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('labels'), 'exclude': lambda f: f is None }})
    r"""Optional. The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""Output only. The resource name of the batch."""  
    operation: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('operation'), 'exclude': lambda f: f is None }})
    r"""Output only. The resource name of the operation associated with this batch."""  
    pyspark_batch: Optional[shared_pysparkbatch.PySparkBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pysparkBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload."""  
    runtime_config: Optional[shared_runtimeconfig.RuntimeConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('runtimeConfig'), 'exclude': lambda f: f is None }})
    r"""Runtime configuration for a workload."""  
    runtime_info: Optional[shared_runtimeinfo.RuntimeInfo] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('runtimeInfo'), 'exclude': lambda f: f is None }})
    r"""Runtime information about workload execution."""  
    spark_batch: Optional[shared_sparkbatch.SparkBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running an Apache Spark (https://spark.apache.org/) batch workload."""  
    spark_r_batch: Optional[shared_sparkrbatch.SparkRBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkRBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload."""  
    spark_sql_batch: Optional[shared_sparksqlbatch.SparkSQLBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkSqlBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload."""  
    state: Optional[BatchStateEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('state'), 'exclude': lambda f: f is None }})
    r"""Output only. The state of the batch."""  
    state_history: Optional[list[shared_statehistory.StateHistory]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('stateHistory'), 'exclude': lambda f: f is None }})
    r"""Output only. Historical state information for the batch."""  
    state_message: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('stateMessage'), 'exclude': lambda f: f is None }})
    r"""Output only. Batch state details, such as a failure description if the state is FAILED."""  
    state_time: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('stateTime'), 'exclude': lambda f: f is None }})
    r"""Output only. The time when the batch entered a current state."""  
    uuid: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('uuid'), 'exclude': lambda f: f is None }})
    r"""Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BatchInput:
    r"""A representation of a batch workload in the service."""
    
    environment_config: Optional[shared_environmentconfig.EnvironmentConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('environmentConfig'), 'exclude': lambda f: f is None }})
    r"""Environment configuration for a workload."""  
    labels: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('labels'), 'exclude': lambda f: f is None }})
    r"""Optional. The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch."""  
    pyspark_batch: Optional[shared_pysparkbatch.PySparkBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pysparkBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload."""  
    runtime_config: Optional[shared_runtimeconfig.RuntimeConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('runtimeConfig'), 'exclude': lambda f: f is None }})
    r"""Runtime configuration for a workload."""  
    runtime_info: Optional[shared_runtimeinfo.RuntimeInfoInput] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('runtimeInfo'), 'exclude': lambda f: f is None }})
    r"""Runtime information about workload execution."""  
    spark_batch: Optional[shared_sparkbatch.SparkBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running an Apache Spark (https://spark.apache.org/) batch workload."""  
    spark_r_batch: Optional[shared_sparkrbatch.SparkRBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkRBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload."""  
    spark_sql_batch: Optional[shared_sparksqlbatch.SparkSQLBatch] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkSqlBatch'), 'exclude': lambda f: f is None }})
    r"""A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload."""  
    