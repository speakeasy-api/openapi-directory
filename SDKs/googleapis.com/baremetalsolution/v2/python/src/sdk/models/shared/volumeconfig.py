"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import lunrange as shared_lunrange
from ..shared import nfsexport as shared_nfsexport
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional

class VolumeConfigPerformanceTierEnum(str, Enum):
    r"""Performance tier of the Volume. Default is SHARED."""
    VOLUME_PERFORMANCE_TIER_UNSPECIFIED = 'VOLUME_PERFORMANCE_TIER_UNSPECIFIED'
    VOLUME_PERFORMANCE_TIER_SHARED = 'VOLUME_PERFORMANCE_TIER_SHARED'
    VOLUME_PERFORMANCE_TIER_ASSIGNED = 'VOLUME_PERFORMANCE_TIER_ASSIGNED'
    VOLUME_PERFORMANCE_TIER_HT = 'VOLUME_PERFORMANCE_TIER_HT'

class VolumeConfigProtocolEnum(str, Enum):
    r"""Volume protocol."""
    PROTOCOL_UNSPECIFIED = 'PROTOCOL_UNSPECIFIED'
    PROTOCOL_FC = 'PROTOCOL_FC'
    PROTOCOL_NFS = 'PROTOCOL_NFS'

class VolumeConfigTypeEnum(str, Enum):
    r"""The type of this Volume."""
    TYPE_UNSPECIFIED = 'TYPE_UNSPECIFIED'
    FLASH = 'FLASH'
    DISK = 'DISK'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class VolumeConfigInput:
    r"""Configuration parameters for a new volume."""
    
    gcp_service: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('gcpService'), 'exclude': lambda f: f is None }})
    r"""The GCP service of the storage volume. Available gcp_service are in https://cloud.google.com/bare-metal/docs/bms-planning."""  
    id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('id'), 'exclude': lambda f: f is None }})
    r"""A transient unique identifier to identify a volume within an ProvisioningConfig request."""  
    lun_ranges: Optional[list[shared_lunrange.LunRange]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lunRanges'), 'exclude': lambda f: f is None }})
    r"""LUN ranges to be configured. Set only when protocol is PROTOCOL_FC."""  
    machine_ids: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('machineIds'), 'exclude': lambda f: f is None }})
    r"""Machine ids connected to this volume. Set only when protocol is PROTOCOL_FC."""  
    nfs_exports: Optional[list[shared_nfsexport.NfsExport]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('nfsExports'), 'exclude': lambda f: f is None }})
    r"""NFS exports. Set only when protocol is PROTOCOL_NFS."""  
    performance_tier: Optional[VolumeConfigPerformanceTierEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('performanceTier'), 'exclude': lambda f: f is None }})
    r"""Performance tier of the Volume. Default is SHARED."""  
    protocol: Optional[VolumeConfigProtocolEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('protocol'), 'exclude': lambda f: f is None }})
    r"""Volume protocol."""  
    size_gb: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sizeGb'), 'exclude': lambda f: f is None }})
    r"""The requested size of this volume, in GB."""  
    snapshots_enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('snapshotsEnabled'), 'exclude': lambda f: f is None }})
    r"""Whether snapshots should be enabled."""  
    storage_aggregate_pool: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('storageAggregatePool'), 'exclude': lambda f: f is None }})
    r"""Input only. Name of the storage aggregate pool to allocate the volume in. Can be used only for VOLUME_PERFORMANCE_TIER_ASSIGNED volumes."""  
    type: Optional[VolumeConfigTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""The type of this Volume."""  
    user_note: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('userNote'), 'exclude': lambda f: f is None }})
    r"""User note field, it can be used by customers to add additional information for the BMS Ops team ."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class VolumeConfig:
    r"""Configuration parameters for a new volume."""
    
    gcp_service: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('gcpService'), 'exclude': lambda f: f is None }})
    r"""The GCP service of the storage volume. Available gcp_service are in https://cloud.google.com/bare-metal/docs/bms-planning."""  
    id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('id'), 'exclude': lambda f: f is None }})
    r"""A transient unique identifier to identify a volume within an ProvisioningConfig request."""  
    lun_ranges: Optional[list[shared_lunrange.LunRange]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lunRanges'), 'exclude': lambda f: f is None }})
    r"""LUN ranges to be configured. Set only when protocol is PROTOCOL_FC."""  
    machine_ids: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('machineIds'), 'exclude': lambda f: f is None }})
    r"""Machine ids connected to this volume. Set only when protocol is PROTOCOL_FC."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""Output only. The name of the volume config."""  
    nfs_exports: Optional[list[shared_nfsexport.NfsExport]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('nfsExports'), 'exclude': lambda f: f is None }})
    r"""NFS exports. Set only when protocol is PROTOCOL_NFS."""  
    performance_tier: Optional[VolumeConfigPerformanceTierEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('performanceTier'), 'exclude': lambda f: f is None }})
    r"""Performance tier of the Volume. Default is SHARED."""  
    protocol: Optional[VolumeConfigProtocolEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('protocol'), 'exclude': lambda f: f is None }})
    r"""Volume protocol."""  
    size_gb: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sizeGb'), 'exclude': lambda f: f is None }})
    r"""The requested size of this volume, in GB."""  
    snapshots_enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('snapshotsEnabled'), 'exclude': lambda f: f is None }})
    r"""Whether snapshots should be enabled."""  
    storage_aggregate_pool: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('storageAggregatePool'), 'exclude': lambda f: f is None }})
    r"""Input only. Name of the storage aggregate pool to allocate the volume in. Can be used only for VOLUME_PERFORMANCE_TIER_ASSIGNED volumes."""  
    type: Optional[VolumeConfigTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""The type of this Volume."""  
    user_note: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('userNote'), 'exclude': lambda f: f is None }})
    r"""User note field, it can be used by customers to add additional information for the BMS Ops team ."""  
    