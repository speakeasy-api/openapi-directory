"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class RankingMetrics:
    r"""Evaluation metrics used by weighted-ALS models specified by feedback_type=implicit."""
    
    average_rank: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('averageRank'), 'exclude': lambda f: f is None }})
    r"""Determines the goodness of a ranking by computing the percentile rank from the predicted confidence and dividing it by the original rank."""  
    mean_average_precision: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('meanAveragePrecision'), 'exclude': lambda f: f is None }})
    r"""Calculates a precision per user for all the items by ranking them and then averages all the precisions across all the users."""  
    mean_squared_error: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('meanSquaredError'), 'exclude': lambda f: f is None }})
    r"""Similar to the mean squared error computed in regression and explicit recommendation models except instead of computing the rating directly, the output from evaluate is computed against a preference which is 1 or 0 depending on if the rating exists or not."""  
    normalized_discounted_cumulative_gain: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('normalizedDiscountedCumulativeGain'), 'exclude': lambda f: f is None }})
    r"""A metric to determine the goodness of a ranking calculated from the predicted confidence by comparing it to an ideal rank measured by the original ratings."""  
    