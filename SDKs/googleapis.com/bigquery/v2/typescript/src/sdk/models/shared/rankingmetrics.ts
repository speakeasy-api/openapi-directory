/*
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

import { SpeakeasyBase, SpeakeasyMetadata } from "../../../internal/utils";
import { Expose } from "class-transformer";

/**
 * Evaluation metrics used by weighted-ALS models specified by feedback_type=implicit.
 */
export class RankingMetrics extends SpeakeasyBase {
  /**
   * Determines the goodness of a ranking by computing the percentile rank from the predicted confidence and dividing it by the original rank.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "averageRank" })
  averageRank?: number;

  /**
   * Calculates a precision per user for all the items by ranking them and then averages all the precisions across all the users.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "meanAveragePrecision" })
  meanAveragePrecision?: number;

  /**
   * Similar to the mean squared error computed in regression and explicit recommendation models except instead of computing the rating directly, the output from evaluate is computed against a preference which is 1 or 0 depending on if the rating exists or not.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "meanSquaredError" })
  meanSquaredError?: number;

  /**
   * A metric to determine the goodness of a ranking calculated from the predicted confidence by comparing it to an ideal rank measured by the original ratings.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "normalizedDiscountedCumulativeGain" })
  normalizedDiscountedCumulativeGain?: number;
}
