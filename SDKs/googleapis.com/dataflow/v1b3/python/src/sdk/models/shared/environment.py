"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import debugoptions as shared_debugoptions
from ..shared import workerpool as shared_workerpool
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Any, Optional

class EnvironmentFlexResourceSchedulingGoalEnum(str, Enum):
    r"""Which Flexible Resource Scheduling mode to run in."""
    FLEXRS_UNSPECIFIED = 'FLEXRS_UNSPECIFIED'
    FLEXRS_SPEED_OPTIMIZED = 'FLEXRS_SPEED_OPTIMIZED'
    FLEXRS_COST_OPTIMIZED = 'FLEXRS_COST_OPTIMIZED'

class EnvironmentShuffleModeEnum(str, Enum):
    r"""Output only. The shuffle mode used for the job."""
    SHUFFLE_MODE_UNSPECIFIED = 'SHUFFLE_MODE_UNSPECIFIED'
    VM_BASED = 'VM_BASED'
    SERVICE_BASED = 'SERVICE_BASED'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class Environment:
    r"""Describes the environment in which a Dataflow Job runs."""
    
    cluster_manager_api_service: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('clusterManagerApiService'), 'exclude': lambda f: f is None }})
    r"""The type of cluster manager API to use. If unknown or unspecified, the service will attempt to choose a reasonable default. This should be in the form of the API service name, e.g. \\"compute.googleapis.com\\"."""  
    dataset: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dataset'), 'exclude': lambda f: f is None }})
    r"""The dataset for the current project where various workflow related tables are stored. The supported resource type is: Google BigQuery: bigquery.googleapis.com/{dataset}"""  
    debug_options: Optional[shared_debugoptions.DebugOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('debugOptions'), 'exclude': lambda f: f is None }})
    r"""Describes any options that have an effect on the debugging of pipelines."""  
    experiments: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('experiments'), 'exclude': lambda f: f is None }})
    r"""The list of experiments to enable. This field should be used for SDK related experiments and not for service related experiments. The proper field for service related experiments is service_options."""  
    flex_resource_scheduling_goal: Optional[EnvironmentFlexResourceSchedulingGoalEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('flexResourceSchedulingGoal'), 'exclude': lambda f: f is None }})
    r"""Which Flexible Resource Scheduling mode to run in."""  
    internal_experiments: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('internalExperiments'), 'exclude': lambda f: f is None }})
    r"""Experimental settings."""  
    sdk_pipeline_options: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sdkPipelineOptions'), 'exclude': lambda f: f is None }})
    r"""The Cloud Dataflow SDK pipeline options specified by the user. These options are passed through the service and are used to recreate the SDK pipeline options on the worker in a language agnostic and platform independent way."""  
    service_account_email: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceAccountEmail'), 'exclude': lambda f: f is None }})
    r"""Identity to run virtual machines as. Defaults to the default account."""  
    service_kms_key_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceKmsKeyName'), 'exclude': lambda f: f is None }})
    r"""If set, contains the Cloud KMS key identifier used to encrypt data at rest, AKA a Customer Managed Encryption Key (CMEK). Format: projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY"""  
    service_options: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceOptions'), 'exclude': lambda f: f is None }})
    r"""The list of service options to enable. This field should be used for service related experiments only. These experiments, when graduating to GA, should be replaced by dedicated fields or become default (i.e. always on)."""  
    shuffle_mode: Optional[EnvironmentShuffleModeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('shuffleMode'), 'exclude': lambda f: f is None }})
    r"""Output only. The shuffle mode used for the job."""  
    temp_storage_prefix: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tempStoragePrefix'), 'exclude': lambda f: f is None }})
    r"""The prefix of the resources the system should use for temporary storage. The system will append the suffix \\"/temp-{JOBNAME} to this resource prefix, where {JOBNAME} is the value of the job_name field. The resulting bucket and object prefix is used as the prefix of the resources used to store temporary data needed during the job execution. NOTE: This will override the value in taskrunner_settings. The supported resource type is: Google Cloud Storage: storage.googleapis.com/{bucket}/{object} bucket.storage.googleapis.com/{object}"""  
    user_agent: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('userAgent'), 'exclude': lambda f: f is None }})
    r"""A description of the process that generated the request."""  
    version: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('version'), 'exclude': lambda f: f is None }})
    r"""A structure describing which components and their versions of the service are required in order to run the job."""  
    worker_pools: Optional[list[shared_workerpool.WorkerPool]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workerPools'), 'exclude': lambda f: f is None }})
    r"""The worker pools. At least one \\"harness\\" worker pool must be specified in order for the job to have workers."""  
    worker_region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workerRegion'), 'exclude': lambda f: f is None }})
    r"""The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \\"us-west1\\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region."""  
    worker_zone: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workerZone'), 'exclude': lambda f: f is None }})
    r"""The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \\"us-west1-a\\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class EnvironmentInput:
    r"""Describes the environment in which a Dataflow Job runs."""
    
    cluster_manager_api_service: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('clusterManagerApiService'), 'exclude': lambda f: f is None }})
    r"""The type of cluster manager API to use. If unknown or unspecified, the service will attempt to choose a reasonable default. This should be in the form of the API service name, e.g. \\"compute.googleapis.com\\"."""  
    dataset: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dataset'), 'exclude': lambda f: f is None }})
    r"""The dataset for the current project where various workflow related tables are stored. The supported resource type is: Google BigQuery: bigquery.googleapis.com/{dataset}"""  
    debug_options: Optional[shared_debugoptions.DebugOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('debugOptions'), 'exclude': lambda f: f is None }})
    r"""Describes any options that have an effect on the debugging of pipelines."""  
    experiments: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('experiments'), 'exclude': lambda f: f is None }})
    r"""The list of experiments to enable. This field should be used for SDK related experiments and not for service related experiments. The proper field for service related experiments is service_options."""  
    flex_resource_scheduling_goal: Optional[EnvironmentFlexResourceSchedulingGoalEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('flexResourceSchedulingGoal'), 'exclude': lambda f: f is None }})
    r"""Which Flexible Resource Scheduling mode to run in."""  
    internal_experiments: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('internalExperiments'), 'exclude': lambda f: f is None }})
    r"""Experimental settings."""  
    sdk_pipeline_options: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sdkPipelineOptions'), 'exclude': lambda f: f is None }})
    r"""The Cloud Dataflow SDK pipeline options specified by the user. These options are passed through the service and are used to recreate the SDK pipeline options on the worker in a language agnostic and platform independent way."""  
    service_account_email: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceAccountEmail'), 'exclude': lambda f: f is None }})
    r"""Identity to run virtual machines as. Defaults to the default account."""  
    service_kms_key_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceKmsKeyName'), 'exclude': lambda f: f is None }})
    r"""If set, contains the Cloud KMS key identifier used to encrypt data at rest, AKA a Customer Managed Encryption Key (CMEK). Format: projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY"""  
    service_options: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceOptions'), 'exclude': lambda f: f is None }})
    r"""The list of service options to enable. This field should be used for service related experiments only. These experiments, when graduating to GA, should be replaced by dedicated fields or become default (i.e. always on)."""  
    temp_storage_prefix: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tempStoragePrefix'), 'exclude': lambda f: f is None }})
    r"""The prefix of the resources the system should use for temporary storage. The system will append the suffix \\"/temp-{JOBNAME} to this resource prefix, where {JOBNAME} is the value of the job_name field. The resulting bucket and object prefix is used as the prefix of the resources used to store temporary data needed during the job execution. NOTE: This will override the value in taskrunner_settings. The supported resource type is: Google Cloud Storage: storage.googleapis.com/{bucket}/{object} bucket.storage.googleapis.com/{object}"""  
    user_agent: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('userAgent'), 'exclude': lambda f: f is None }})
    r"""A description of the process that generated the request."""  
    version: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('version'), 'exclude': lambda f: f is None }})
    r"""A structure describing which components and their versions of the service are required in order to run the job."""  
    worker_pools: Optional[list[shared_workerpool.WorkerPool]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workerPools'), 'exclude': lambda f: f is None }})
    r"""The worker pools. At least one \\"harness\\" worker pool must be specified in order for the job to have workers."""  
    worker_region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workerRegion'), 'exclude': lambda f: f is None }})
    r"""The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \\"us-west1\\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region."""  
    worker_zone: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workerZone'), 'exclude': lambda f: f is None }})
    r"""The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \\"us-west1-a\\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity."""  
    