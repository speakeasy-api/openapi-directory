"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import dateutil.parser
from dataclasses_json import Undefined, dataclass_json
from datetime import datetime
from marshmallow import fields
from sdk import utils


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class JobScheduleStatistics:
    r"""Resource usage statistics for a job schedule."""
    
    kernel_cpu_time: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('kernelCPUTime') }})
    r"""The total kernel mode CPU time (summed across all cores and all compute nodes) consumed by all tasks in all jobs created under the schedule."""  
    last_update_time: datetime = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lastUpdateTime'), 'encoder': utils.datetimeisoformat(False), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso') }})
    r"""The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime."""  
    num_failed_tasks: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('numFailedTasks') }})
    r"""The total number of tasks that failed during the given time range in jobs created under the schedule. A task fails if it exhausts its maximum retry count without returning exit code 0."""  
    num_succeeded_tasks: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('numSucceededTasks') }})
    r"""The total number of tasks successfully completed during the given time range in jobs created under the schedule. A task completes successfully if it returns exit code 0."""  
    num_task_retries: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('numTaskRetries') }})
    r"""The total number of retries during the given time range on all tasks in all jobs created under the schedule."""  
    read_io_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('readIOGiB') }})
    r"""The total gibibytes read from disk by all tasks in all jobs created under the schedule."""  
    read_i_ops: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('readIOps') }})
    r"""The total number of disk read operations made by all tasks in all jobs created under the schedule."""  
    start_time: datetime = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('startTime'), 'encoder': utils.datetimeisoformat(False), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso') }})
    r"""The start time of the time range covered by the statistics."""  
    url: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('url') }})
    r"""The URL of the statistics."""  
    user_cpu_time: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('userCPUTime') }})
    r"""The total user mode CPU time (summed across all cores and all compute nodes) consumed by all tasks in all jobs created under the schedule."""  
    wait_time: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('waitTime') }})
    r"""The total wait time of all tasks in all jobs created under the schedule. The wait time for a task is defined as the elapsed time between the creation of the task and the start of task execution. (If the task is retried due to failures, the wait time is the time to the most recent task execution.)"""  
    wall_clock_time: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('wallClockTime') }})
    r"""The total wall clock time of all the tasks in all the jobs created under the schedule."""  
    write_io_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('writeIOGiB') }})
    r"""The total gibibytes written to disk by all tasks in all jobs created under the schedule."""  
    write_i_ops: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('writeIOps') }})
    r"""The total number of disk write operations made by all tasks in all jobs created under the schedule."""  
    