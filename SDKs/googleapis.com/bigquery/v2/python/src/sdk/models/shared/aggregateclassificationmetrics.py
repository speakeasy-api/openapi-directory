"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class AggregateClassificationMetrics:
    r"""Aggregate metrics for classification/classifier models. For multi-class models, the metrics are either macro-averaged or micro-averaged. When macro-averaged, the metrics are calculated for each label and then an unweighted average is taken of those values. When micro-averaged, the metric is calculated globally by counting the total number of correctly predicted rows."""
    
    accuracy: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('accuracy'), 'exclude': lambda f: f is None }})
    r"""Accuracy is the fraction of predictions given the correct label. For multiclass this is a micro-averaged metric."""  
    f1_score: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('f1Score'), 'exclude': lambda f: f is None }})
    r"""The F1 score is an average of recall and precision. For multiclass this is a macro-averaged metric."""  
    log_loss: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logLoss'), 'exclude': lambda f: f is None }})
    r"""Logarithmic Loss. For multiclass this is a macro-averaged metric."""  
    precision: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('precision'), 'exclude': lambda f: f is None }})
    r"""Precision is the fraction of actual positive predictions that had positive actual labels. For multiclass this is a macro-averaged metric treating each class as a binary classifier."""  
    recall: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('recall'), 'exclude': lambda f: f is None }})
    r"""Recall is the fraction of actual positive labels that were given a positive prediction. For multiclass this is a macro-averaged metric."""  
    roc_auc: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('rocAuc'), 'exclude': lambda f: f is None }})
    r"""Area Under a ROC Curve. For multiclass this is a macro-averaged metric."""  
    threshold: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('threshold'), 'exclude': lambda f: f is None }})
    r"""Threshold at which the metrics are computed. For binary classification models this is the positive class threshold. For multi-class classfication models this is the confidence threshold."""  
    