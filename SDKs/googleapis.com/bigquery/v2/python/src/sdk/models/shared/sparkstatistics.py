"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import sparklogginginfo as shared_sparklogginginfo
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class SparkStatistics:
    
    endpoints: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('endpoints'), 'exclude': lambda f: f is None }})
    r"""[Output-only] Endpoints generated for the Spark job."""  
    logging_info: Optional[shared_sparklogginginfo.SparkLoggingInfo] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logging_info'), 'exclude': lambda f: f is None }})  
    spark_job_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('spark_job_id'), 'exclude': lambda f: f is None }})
    r"""[Output-only] Spark job id if a Spark job is created successfully."""  
    spark_job_location: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('spark_job_location'), 'exclude': lambda f: f is None }})
    r"""[Output-only] Location where the Spark job is executed."""  
    