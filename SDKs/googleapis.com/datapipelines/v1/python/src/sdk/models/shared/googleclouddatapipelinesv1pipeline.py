"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import googleclouddatapipelinesv1schedulespec as shared_googleclouddatapipelinesv1schedulespec
from ..shared import googleclouddatapipelinesv1workload as shared_googleclouddatapipelinesv1workload
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional

class GoogleCloudDatapipelinesV1PipelineStateEnum(str, Enum):
    r"""Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests."""
    STATE_UNSPECIFIED = 'STATE_UNSPECIFIED'
    STATE_RESUMING = 'STATE_RESUMING'
    STATE_ACTIVE = 'STATE_ACTIVE'
    STATE_STOPPING = 'STATE_STOPPING'
    STATE_ARCHIVED = 'STATE_ARCHIVED'
    STATE_PAUSED = 'STATE_PAUSED'

class GoogleCloudDatapipelinesV1PipelineTypeEnum(str, Enum):
    r"""Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline."""
    PIPELINE_TYPE_UNSPECIFIED = 'PIPELINE_TYPE_UNSPECIFIED'
    PIPELINE_TYPE_BATCH = 'PIPELINE_TYPE_BATCH'
    PIPELINE_TYPE_STREAMING = 'PIPELINE_TYPE_STREAMING'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class GoogleCloudDatapipelinesV1Pipeline:
    r"""The main pipeline entity and all the necessary metadata for launching and managing linked jobs."""
    
    create_time: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('createTime'), 'exclude': lambda f: f is None }})
    r"""Output only. Immutable. The timestamp when the pipeline was initially created. Set by the Data Pipelines service."""  
    display_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('displayName'), 'exclude': lambda f: f is None }})
    r"""Required. The display name of the pipeline. It can contain only letters ([A-Za-z]), numbers ([0-9]), hyphens (-), and underscores (_)."""  
    job_count: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobCount'), 'exclude': lambda f: f is None }})
    r"""Output only. Number of jobs."""  
    last_update_time: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lastUpdateTime'), 'exclude': lambda f: f is None }})
    r"""Output only. Immutable. The timestamp when the pipeline was last modified. Set by the Data Pipelines service."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""The pipeline name. For example: `projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID`. * `PROJECT_ID` can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see [Identifying projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects). * `LOCATION_ID` is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling `google.cloud.location.Locations.ListLocations`. Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in [App Engine regions](https://cloud.google.com/about/locations#region). * `PIPELINE_ID` is the ID of the pipeline. Must be unique for the selected project and location."""  
    pipeline_sources: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pipelineSources'), 'exclude': lambda f: f is None }})
    r"""Immutable. The sources of the pipeline (for example, Dataplex). The keys and values are set by the corresponding sources during pipeline creation."""  
    schedule_info: Optional[shared_googleclouddatapipelinesv1schedulespec.GoogleCloudDatapipelinesV1ScheduleSpec] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('scheduleInfo'), 'exclude': lambda f: f is None }})
    r"""Details of the schedule the pipeline runs on."""  
    scheduler_service_account_email: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('schedulerServiceAccountEmail'), 'exclude': lambda f: f is None }})
    r"""Optional. A service account email to be used with the Cloud Scheduler job. If not specified, the default compute engine service account will be used."""  
    state: Optional[GoogleCloudDatapipelinesV1PipelineStateEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('state'), 'exclude': lambda f: f is None }})
    r"""Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests."""  
    type: Optional[GoogleCloudDatapipelinesV1PipelineTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline."""  
    workload: Optional[shared_googleclouddatapipelinesv1workload.GoogleCloudDatapipelinesV1Workload] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workload'), 'exclude': lambda f: f is None }})
    r"""Workload details for creating the pipeline jobs."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class GoogleCloudDatapipelinesV1PipelineInput:
    r"""The main pipeline entity and all the necessary metadata for launching and managing linked jobs."""
    
    display_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('displayName'), 'exclude': lambda f: f is None }})
    r"""Required. The display name of the pipeline. It can contain only letters ([A-Za-z]), numbers ([0-9]), hyphens (-), and underscores (_)."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""The pipeline name. For example: `projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID`. * `PROJECT_ID` can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see [Identifying projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects). * `LOCATION_ID` is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling `google.cloud.location.Locations.ListLocations`. Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in [App Engine regions](https://cloud.google.com/about/locations#region). * `PIPELINE_ID` is the ID of the pipeline. Must be unique for the selected project and location."""  
    pipeline_sources: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pipelineSources'), 'exclude': lambda f: f is None }})
    r"""Immutable. The sources of the pipeline (for example, Dataplex). The keys and values are set by the corresponding sources during pipeline creation."""  
    schedule_info: Optional[shared_googleclouddatapipelinesv1schedulespec.GoogleCloudDatapipelinesV1ScheduleSpecInput] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('scheduleInfo'), 'exclude': lambda f: f is None }})
    r"""Details of the schedule the pipeline runs on."""  
    scheduler_service_account_email: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('schedulerServiceAccountEmail'), 'exclude': lambda f: f is None }})
    r"""Optional. A service account email to be used with the Cloud Scheduler job. If not specified, the default compute engine service account will be used."""  
    state: Optional[GoogleCloudDatapipelinesV1PipelineStateEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('state'), 'exclude': lambda f: f is None }})
    r"""Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests."""  
    type: Optional[GoogleCloudDatapipelinesV1PipelineTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline."""  
    workload: Optional[shared_googleclouddatapipelinesv1workload.GoogleCloudDatapipelinesV1Workload] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workload'), 'exclude': lambda f: f is None }})
    r"""Workload details for creating the pipeline jobs."""  
    