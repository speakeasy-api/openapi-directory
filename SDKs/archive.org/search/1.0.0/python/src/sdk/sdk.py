"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

import requests as requests_http
from . import utils
from sdk.models import operations, shared
from typing import Optional

SERVERS = [
    "https://api.archive.org",
]
"""Contains the list of servers available to the SDK"""

class SDK:
    r"""API for Internet Archive's Search-related services"""

    _client: requests_http.Session
    _security_client: requests_http.Session
    _server_url: str = SERVERS[0]
    _language: str = "python"
    _sdk_version: str = "0.0.1"
    _gen_version: str = "internal"

    def __init__(self,
                 server_url: str = None,
                 url_params: dict[str, str] = None,
                 client: requests_http.Session = None
                 ) -> None:
        """Instantiates the SDK configuring it with the provided parameters.
        
        :param server_url: The server URL to use for all operations
        :type server_url: str
        :param url_params: Parameters to optionally template the server URL with
        :type url_params: dict[str, str]
        :param client: The requests.Session HTTP client to use for all operations
        :type client: requests_http.Session        
        """
        self._client = requests_http.Session()
        
        
        if server_url is not None:
            if url_params is not None:
                self._server_url = utils.template_url(server_url, url_params)
            else:
                self._server_url = server_url

        if client is not None:
            self._client = client
        
        self._security_client = self._client
        

        
    
    
    def get_search_v1_fields(self, request: operations.GetSearchV1FieldsRequest) -> operations.GetSearchV1FieldsResponse:
        r"""Fields that can be requested"""
        base_url = self._server_url
        
        url = base_url.removesuffix('/') + '/search/v1/fields'
        
        query_params = utils.get_query_params(operations.GetSearchV1FieldsRequest, request)
        
        client = self._client
        
        http_res = client.request('GET', url, params=query_params)
        content_type = http_res.headers.get('Content-Type')

        res = operations.GetSearchV1FieldsResponse(status_code=http_res.status_code, content_type=content_type, raw_response=http_res)
        
        if http_res.status_code == 200:
            if utils.match_content_type(content_type, 'application/javascript'):
                res.body = http_res.content
            if utils.match_content_type(content_type, 'application/json'):
                out = utils.unmarshal_json(http_res.text, Optional[list[str]])
                res.fields_ = out

        return res

    def get_search_v1_organic(self, request: operations.GetSearchV1OrganicRequest) -> operations.GetSearchV1OrganicResponse:
        r"""Return relevance-based results from search queries"""
        base_url = self._server_url
        
        url = base_url.removesuffix('/') + '/search/v1/organic'
        
        query_params = utils.get_query_params(operations.GetSearchV1OrganicRequest, request)
        
        client = self._client
        
        http_res = client.request('GET', url, params=query_params)
        content_type = http_res.headers.get('Content-Type')

        res = operations.GetSearchV1OrganicResponse(status_code=http_res.status_code, content_type=content_type, raw_response=http_res)
        
        if http_res.status_code == 200:
            if utils.match_content_type(content_type, 'application/javascript'):
                res.body = http_res.content
            if utils.match_content_type(content_type, 'application/json'):
                out = utils.unmarshal_json(http_res.text, Optional[shared.OrganicResult])
                res.organic_result = out
        else:
            if utils.match_content_type(content_type, 'application/javascript'):
                res.body = http_res.content
            if utils.match_content_type(content_type, 'application/json'):
                out = utils.unmarshal_json(http_res.text, Optional[shared.Error])
                res.error = out

        return res

    def get_search_v1_scrape(self, request: operations.GetSearchV1ScrapeRequest) -> operations.GetSearchV1ScrapeResponse:
        r"""Scrape search results from Internet Archive, allowing a scrolling cursor"""
        base_url = self._server_url
        
        url = base_url.removesuffix('/') + '/search/v1/scrape'
        
        query_params = utils.get_query_params(operations.GetSearchV1ScrapeRequest, request)
        
        client = self._client
        
        http_res = client.request('GET', url, params=query_params)
        content_type = http_res.headers.get('Content-Type')

        res = operations.GetSearchV1ScrapeResponse(status_code=http_res.status_code, content_type=content_type, raw_response=http_res)
        
        if http_res.status_code == 200:
            if utils.match_content_type(content_type, 'application/javascript'):
                res.body = http_res.content
            if utils.match_content_type(content_type, 'application/json'):
                out = utils.unmarshal_json(http_res.text, Optional[shared.ScrapeResult])
                res.scrape_result = out
        else:
            if utils.match_content_type(content_type, 'application/javascript'):
                res.body = http_res.content
            if utils.match_content_type(content_type, 'application/json'):
                out = utils.unmarshal_json(http_res.text, Optional[shared.Error])
                res.error = out

        return res

    