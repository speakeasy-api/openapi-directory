"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional

class RemoteServerEntityAuthStatusEnum(str, Enum):
    r"""Either `in_setup` or `complete`"""
    NOT_APPLICABLE = 'not_applicable'
    IN_SETUP = 'in_setup'
    COMPLETE = 'complete'
    REAUTHENTICATE = 'reauthenticate'

class RemoteServerEntityFilesAgentPermissionSetEnum(str, Enum):
    r"""Local permissions for files agent. read_only, write_only, or read_write"""
    READ_WRITE = 'read_write'
    READ_ONLY = 'read_only'
    WRITE_ONLY = 'write_only'

class RemoteServerEntityOneDriveAccountTypeEnum(str, Enum):
    r"""Either personal or business_other account types"""
    PERSONAL = 'personal'
    BUSINESS_OTHER = 'business_other'

class RemoteServerEntityServerCertificateEnum(str, Enum):
    r"""Remote server certificate"""
    REQUIRE_MATCH = 'require_match'
    ALLOW_ANY = 'allow_any'

class RemoteServerEntityServerTypeEnum(str, Enum):
    r"""Remote server type."""
    FTP = 'ftp'
    SFTP = 'sftp'
    S3 = 's3'
    GOOGLE_CLOUD_STORAGE = 'google_cloud_storage'
    WEBDAV = 'webdav'
    WASABI = 'wasabi'
    BACKBLAZE_B2 = 'backblaze_b2'
    ONE_DRIVE = 'one_drive'
    RACKSPACE = 'rackspace'
    BOX = 'box'
    DROPBOX = 'dropbox'
    GOOGLE_DRIVE = 'google_drive'
    AZURE = 'azure'
    SHAREPOINT = 'sharepoint'
    S3_COMPATIBLE = 's3_compatible'
    AZURE_FILES = 'azure_files'
    FILES_AGENT = 'files_agent'
    FILEBASE = 'filebase'

class RemoteServerEntitySslEnum(str, Enum):
    r"""Should we require SSL?"""
    IF_AVAILABLE = 'if_available'
    REQUIRE = 'require'
    REQUIRE_IMPLICIT = 'require_implicit'
    NEVER = 'never'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class RemoteServerEntity:
    r"""Create Remote Server"""
    
    auth_account_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('auth_account_name'), 'exclude': lambda f: f is None }})
    r"""Describes the authorized account"""  
    auth_setup_link: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('auth_setup_link'), 'exclude': lambda f: f is None }})
    r"""Returns link to login with an Oauth provider"""  
    auth_status: Optional[RemoteServerEntityAuthStatusEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('auth_status'), 'exclude': lambda f: f is None }})
    r"""Either `in_setup` or `complete`"""  
    authentication_method: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('authentication_method'), 'exclude': lambda f: f is None }})
    r"""Type of authentication method"""  
    aws_access_key: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('aws_access_key'), 'exclude': lambda f: f is None }})
    r"""AWS Access Key."""  
    azure_blob_storage_account: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('azure_blob_storage_account'), 'exclude': lambda f: f is None }})
    r"""Azure Blob Storage Account name"""  
    azure_blob_storage_container: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('azure_blob_storage_container'), 'exclude': lambda f: f is None }})
    r"""Azure Blob Storage Container name"""  
    azure_blob_storage_sas_token: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('azure_blob_storage_sas_token'), 'exclude': lambda f: f is None }})
    r"""Shared Access Signature (SAS) token"""  
    azure_files_storage_account: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('azure_files_storage_account'), 'exclude': lambda f: f is None }})
    r"""Azure File Storage Account name"""  
    azure_files_storage_sas_token: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('azure_files_storage_sas_token'), 'exclude': lambda f: f is None }})
    r"""Shared Access Signature (SAS) token"""  
    azure_files_storage_share_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('azure_files_storage_share_name'), 'exclude': lambda f: f is None }})
    r"""Azure File Storage Share name"""  
    backblaze_b2_bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('backblaze_b2_bucket'), 'exclude': lambda f: f is None }})
    r"""Backblaze B2 Cloud Storage Bucket name"""  
    backblaze_b2_s3_endpoint: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('backblaze_b2_s3_endpoint'), 'exclude': lambda f: f is None }})
    r"""Backblaze B2 Cloud Storage S3 Endpoint"""  
    disabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('disabled'), 'exclude': lambda f: f is None }})
    r"""If true, this server has been disabled due to failures.  Make any change or set disabled to false to clear this flag."""  
    enable_dedicated_ips: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('enable_dedicated_ips'), 'exclude': lambda f: f is None }})
    r"""`true` if remote server only accepts connections from dedicated IPs"""  
    filebase_access_key: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('filebase_access_key'), 'exclude': lambda f: f is None }})
    r"""Filebase Access Key."""  
    filebase_bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('filebase_bucket'), 'exclude': lambda f: f is None }})
    r"""Filebase Bucket name"""  
    files_agent_api_token: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('files_agent_api_token'), 'exclude': lambda f: f is None }})
    r"""Files Agent API Token"""  
    files_agent_permission_set: Optional[RemoteServerEntityFilesAgentPermissionSetEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('files_agent_permission_set'), 'exclude': lambda f: f is None }})
    r"""Local permissions for files agent. read_only, write_only, or read_write"""  
    files_agent_root: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('files_agent_root'), 'exclude': lambda f: f is None }})
    r"""Agent local root path"""  
    google_cloud_storage_bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('google_cloud_storage_bucket'), 'exclude': lambda f: f is None }})
    r"""Google Cloud Storage bucket name"""  
    google_cloud_storage_project_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('google_cloud_storage_project_id'), 'exclude': lambda f: f is None }})
    r"""Google Cloud Project ID"""  
    hostname: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('hostname'), 'exclude': lambda f: f is None }})
    r"""Hostname or IP address"""  
    id: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('id'), 'exclude': lambda f: f is None }})
    r"""Remote server ID"""  
    max_connections: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('max_connections'), 'exclude': lambda f: f is None }})
    r"""Max number of parallel connections.  Ignored for S3 connections (we will parallelize these as much as possible)."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""Internal name for your reference"""  
    one_drive_account_type: Optional[RemoteServerEntityOneDriveAccountTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('one_drive_account_type'), 'exclude': lambda f: f is None }})
    r"""Either personal or business_other account types"""  
    pin_to_site_region: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pin_to_site_region'), 'exclude': lambda f: f is None }})
    r"""If true, we will ensure that all communications with this remote server are made through the primary region of the site.  This setting can also be overridden by a sitewide setting which will force it to true."""  
    pinned_region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pinned_region'), 'exclude': lambda f: f is None }})
    r"""If set, all communciations with this remote server are made through the provided region."""  
    port: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('port'), 'exclude': lambda f: f is None }})
    r"""Port for remote server.  Not needed for S3."""  
    rackspace_container: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('rackspace_container'), 'exclude': lambda f: f is None }})
    r"""The name of the container (top level directory) where files will sync."""  
    rackspace_region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('rackspace_region'), 'exclude': lambda f: f is None }})
    r"""Three letter airport code for Rackspace region. See https://support.rackspace.com/how-to/about-regions/"""  
    rackspace_username: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('rackspace_username'), 'exclude': lambda f: f is None }})
    r"""Rackspace username used to login to the Rackspace Cloud Control Panel."""  
    remote_home_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('remote_home_path'), 'exclude': lambda f: f is None }})
    r"""Initial home folder on remote server"""  
    s3_bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_bucket'), 'exclude': lambda f: f is None }})
    r"""S3 bucket name"""  
    s3_compatible_access_key: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_compatible_access_key'), 'exclude': lambda f: f is None }})
    r"""S3-compatible Access Key."""  
    s3_compatible_bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_compatible_bucket'), 'exclude': lambda f: f is None }})
    r"""S3-compatible Bucket name"""  
    s3_compatible_endpoint: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_compatible_endpoint'), 'exclude': lambda f: f is None }})
    r"""S3-compatible endpoint"""  
    s3_compatible_region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_compatible_region'), 'exclude': lambda f: f is None }})
    r"""S3-compatible endpoint"""  
    s3_region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_region'), 'exclude': lambda f: f is None }})
    r"""S3 region"""  
    server_certificate: Optional[RemoteServerEntityServerCertificateEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('server_certificate'), 'exclude': lambda f: f is None }})
    r"""Remote server certificate"""  
    server_host_key: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('server_host_key'), 'exclude': lambda f: f is None }})
    r"""Remote server SSH Host Key. If provided, we will require that the server host key matches the provided key. Uses OpenSSH format similar to what would go into ~/.ssh/known_hosts"""  
    server_type: Optional[RemoteServerEntityServerTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('server_type'), 'exclude': lambda f: f is None }})
    r"""Remote server type."""  
    ssl: Optional[RemoteServerEntitySslEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ssl'), 'exclude': lambda f: f is None }})
    r"""Should we require SSL?"""  
    username: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('username'), 'exclude': lambda f: f is None }})
    r"""Remote server username.  Not needed for S3 buckets."""  
    wasabi_access_key: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('wasabi_access_key'), 'exclude': lambda f: f is None }})
    r"""Wasabi access key."""  
    wasabi_bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('wasabi_bucket'), 'exclude': lambda f: f is None }})
    r"""Wasabi Bucket name"""  
    wasabi_region: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('wasabi_region'), 'exclude': lambda f: f is None }})
    r"""Wasabi region"""  
    