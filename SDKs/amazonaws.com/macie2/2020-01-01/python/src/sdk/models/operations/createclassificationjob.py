"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import requests as requests_http
from ..shared import createclassificationjobresponse as shared_createclassificationjobresponse
from ..shared import monthlyschedule as shared_monthlyschedule
from ..shared import s3bucketcriteriaforjob as shared_s3bucketcriteriaforjob
from ..shared import s3bucketdefinitionforjob as shared_s3bucketdefinitionforjob
from ..shared import scoping as shared_scoping
from ..shared import weeklyschedule as shared_weeklyschedule
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Any, Optional

class CreateClassificationJobRequestBodyJobTypeEnum(str, Enum):
    r"""The schedule for running a classification job. Valid values are:"""
    ONE_TIME = 'ONE_TIME'
    SCHEDULED = 'SCHEDULED'

class CreateClassificationJobRequestBodyManagedDataIdentifierSelectorEnum(str, Enum):
    r"""The selection type that determines which managed data identifiers a classification job uses to analyze data. Valid values are:"""
    ALL = 'ALL'
    EXCLUDE = 'EXCLUDE'
    INCLUDE = 'INCLUDE'
    NONE = 'NONE'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateClassificationJobRequestBodyS3JobDefinition:
    r"""Specifies which S3 buckets contain the objects that a classification job analyzes, and the scope of that analysis. The bucket specification can be static (bucketDefinitions) or dynamic (bucketCriteria). If it's static, the job analyzes objects in the same predefined set of buckets each time the job runs. If it's dynamic, the job analyzes objects in any buckets that match the specified criteria each time the job starts to run."""
    
    bucket_criteria: Optional[shared_s3bucketcriteriaforjob.S3BucketCriteriaForJob] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucketCriteria'), 'exclude': lambda f: f is None }})  
    bucket_definitions: Optional[list[shared_s3bucketdefinitionforjob.S3BucketDefinitionForJob]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucketDefinitions'), 'exclude': lambda f: f is None }})  
    scoping: Optional[shared_scoping.Scoping] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('scoping'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateClassificationJobRequestBodyScheduleFrequency:
    r"""Specifies the recurrence pattern for running a classification job."""
    
    daily_schedule: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dailySchedule'), 'exclude': lambda f: f is None }})  
    monthly_schedule: Optional[shared_monthlyschedule.MonthlySchedule] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('monthlySchedule'), 'exclude': lambda f: f is None }})  
    weekly_schedule: Optional[shared_weeklyschedule.WeeklySchedule] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('weeklySchedule'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateClassificationJobRequestBody:
    
    client_token: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('clientToken') }})
    r"""A unique, case-sensitive token that you provide to ensure the idempotency of the request."""  
    job_type: CreateClassificationJobRequestBodyJobTypeEnum = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobType') }})
    r"""The schedule for running a classification job. Valid values are:"""  
    name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name') }})
    r"""A custom name for the job. The name can contain as many as 500 characters."""  
    s3_job_definition: CreateClassificationJobRequestBodyS3JobDefinition = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3JobDefinition') }})
    r"""Specifies which S3 buckets contain the objects that a classification job analyzes, and the scope of that analysis. The bucket specification can be static (bucketDefinitions) or dynamic (bucketCriteria). If it's static, the job analyzes objects in the same predefined set of buckets each time the job runs. If it's dynamic, the job analyzes objects in any buckets that match the specified criteria each time the job starts to run."""  
    allow_list_ids: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('allowListIds'), 'exclude': lambda f: f is None }})
    r"""An array of unique identifiers, one for each allow list for the job to use when it analyzes data."""  
    custom_data_identifier_ids: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('customDataIdentifierIds'), 'exclude': lambda f: f is None }})
    r"""An array of unique identifiers, one for each custom data identifier for the job to use when it analyzes data. To use only managed data identifiers, don't specify a value for this property and specify a value other than NONE for the managedDataIdentifierSelector property."""  
    description: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('description'), 'exclude': lambda f: f is None }})
    r"""A custom description of the job. The description can contain as many as 200 characters."""  
    initial_run: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('initialRun'), 'exclude': lambda f: f is None }})
    r"""<p>For a recurring job, specifies whether to analyze all existing, eligible objects immediately after the job is created (true). To analyze only those objects that are created or changed after you create the job and before the job's first scheduled run, set this value to false.</p> <p>If you configure the job to run only once, don't specify a value for this property.</p>"""  
    managed_data_identifier_ids: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('managedDataIdentifierIds'), 'exclude': lambda f: f is None }})
    r"""<p>An array of unique identifiers, one for each managed data identifier for the job to include (use) or exclude (not use) when it analyzes data. Inclusion or exclusion depends on the managed data identifier selection type that you specify for the job (managedDataIdentifierSelector).</p> <p>To retrieve a list of valid values for this property, use the ListManagedDataIdentifiers operation.</p>"""  
    managed_data_identifier_selector: Optional[CreateClassificationJobRequestBodyManagedDataIdentifierSelectorEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('managedDataIdentifierSelector'), 'exclude': lambda f: f is None }})
    r"""The selection type that determines which managed data identifiers a classification job uses to analyze data. Valid values are:"""  
    sampling_percentage: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('samplingPercentage'), 'exclude': lambda f: f is None }})
    r"""The sampling depth, as a percentage, for the job to apply when processing objects. This value determines the percentage of eligible objects that the job analyzes. If this value is less than 100, Amazon Macie selects the objects to analyze at random, up to the specified percentage, and analyzes all the data in those objects."""  
    schedule_frequency: Optional[CreateClassificationJobRequestBodyScheduleFrequency] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('scheduleFrequency'), 'exclude': lambda f: f is None }})
    r"""Specifies the recurrence pattern for running a classification job."""  
    tags: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tags'), 'exclude': lambda f: f is None }})
    r"""A string-to-string map of key-value pairs that specifies the tags (keys and values) for an Amazon Macie resource."""  
    

@dataclasses.dataclass
class CreateClassificationJobRequest:
    
    request_body: CreateClassificationJobRequestBody = dataclasses.field(metadata={'request': { 'media_type': 'application/json' }})  
    x_amz_algorithm: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Algorithm', 'style': 'simple', 'explode': False }})  
    x_amz_content_sha256: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Content-Sha256', 'style': 'simple', 'explode': False }})  
    x_amz_credential: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Credential', 'style': 'simple', 'explode': False }})  
    x_amz_date: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Date', 'style': 'simple', 'explode': False }})  
    x_amz_security_token: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Security-Token', 'style': 'simple', 'explode': False }})  
    x_amz_signature: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Signature', 'style': 'simple', 'explode': False }})  
    x_amz_signed_headers: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-SignedHeaders', 'style': 'simple', 'explode': False }})  
    

@dataclasses.dataclass
class CreateClassificationJobResponse:
    
    content_type: str = dataclasses.field()  
    status_code: int = dataclasses.field()  
    access_denied_exception: Optional[Any] = dataclasses.field(default=None)
    r"""AccessDeniedException"""  
    conflict_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ConflictException"""  
    create_classification_job_response: Optional[shared_createclassificationjobresponse.CreateClassificationJobResponse] = dataclasses.field(default=None)
    r"""Success"""  
    internal_server_exception: Optional[Any] = dataclasses.field(default=None)
    r"""InternalServerException"""  
    raw_response: Optional[requests_http.Response] = dataclasses.field(default=None)  
    resource_not_found_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ResourceNotFoundException"""  
    service_quota_exceeded_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ServiceQuotaExceededException"""  
    throttling_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ThrottlingException"""  
    validation_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ValidationException"""  
    