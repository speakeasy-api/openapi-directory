"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import computationtopology as shared_computationtopology
from ..shared import datadiskassignment as shared_datadiskassignment
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class TopologyConfig:
    r"""Global topology of the streaming Dataflow job, including all computations and their sharded locations."""
    
    computations: Optional[list[shared_computationtopology.ComputationTopology]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('computations'), 'exclude': lambda f: f is None }})
    r"""The computations associated with a streaming Dataflow job."""  
    data_disk_assignments: Optional[list[shared_datadiskassignment.DataDiskAssignment]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dataDiskAssignments'), 'exclude': lambda f: f is None }})
    r"""The disks assigned to a streaming Dataflow job."""  
    forwarding_key_bits: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('forwardingKeyBits'), 'exclude': lambda f: f is None }})
    r"""The size (in bits) of keys that will be assigned to source messages."""  
    persistent_state_version: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('persistentStateVersion'), 'exclude': lambda f: f is None }})
    r"""Version number for persistent state."""  
    user_stage_to_computation_name_map: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('userStageToComputationNameMap'), 'exclude': lambda f: f is None }})
    r"""Maps user stage names to stable computation names."""  
    