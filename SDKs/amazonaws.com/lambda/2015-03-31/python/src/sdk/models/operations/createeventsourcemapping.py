"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import dateutil.parser
import requests as requests_http
from ..shared import eventsourcemappingconfiguration as shared_eventsourcemappingconfiguration
from ..shared import filter as shared_filter
from ..shared import fulldocument_enum as shared_fulldocument_enum
from ..shared import functionresponsetype_enum as shared_functionresponsetype_enum
from ..shared import onfailure as shared_onfailure
from ..shared import onsuccess as shared_onsuccess
from ..shared import sourceaccessconfiguration as shared_sourceaccessconfiguration
from dataclasses_json import Undefined, dataclass_json
from datetime import datetime
from enum import Enum
from marshmallow import fields
from sdk import utils
from typing import Any, Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBodyAmazonManagedKafkaEventSourceConfig:
    r"""Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source."""
    
    consumer_group_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ConsumerGroupId'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBodyDestinationConfig:
    r"""A configuration object that specifies the destination of an event after Lambda processes it."""
    
    on_failure: Optional[shared_onfailure.OnFailure] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('OnFailure'), 'exclude': lambda f: f is None }})  
    on_success: Optional[shared_onsuccess.OnSuccess] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('OnSuccess'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBodyDocumentDBEventSourceConfig:
    r"""Specific configuration settings for a DocumentDB event source."""
    
    collection_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CollectionName'), 'exclude': lambda f: f is None }})  
    database_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DatabaseName'), 'exclude': lambda f: f is None }})  
    full_document: Optional[shared_fulldocument_enum.FullDocumentEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FullDocument'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBodyFilterCriteria:
    r"""An object that contains the filters for an event source."""
    
    filters: Optional[list[shared_filter.Filter]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Filters'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBodyScalingConfig:
    r"""(Amazon SQS only) The scaling configuration for the event source. To remove the configuration, pass an empty value."""
    
    maximum_concurrency: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MaximumConcurrency'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBodySelfManagedEventSource:
    r"""The self-managed Apache Kafka cluster for your event source."""
    
    endpoints: Optional[dict[str, list[str]]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Endpoints'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBodySelfManagedKafkaEventSourceConfig:
    r"""Specific configuration settings for a self-managed Apache Kafka event source."""
    
    consumer_group_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ConsumerGroupId'), 'exclude': lambda f: f is None }})  
    
class CreateEventSourceMappingRequestBodyStartingPositionEnum(str, Enum):
    r"""The position in a stream from which to start reading. Required for Amazon Kinesis, Amazon DynamoDB, and Amazon MSK Streams sources. <code>AT_TIMESTAMP</code> is supported only for Amazon Kinesis streams."""
    TRIM_HORIZON = 'TRIM_HORIZON'
    LATEST = 'LATEST'
    AT_TIMESTAMP = 'AT_TIMESTAMP'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateEventSourceMappingRequestBody:
    
    function_name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FunctionName') }})
    r"""<p>The name of the Lambda function.</p> <p class=\\"title\\"> <b>Name formats</b> </p> <ul> <li> <p> <b>Function name</b> – <code>MyFunction</code>.</p> </li> <li> <p> <b>Function ARN</b> – <code>arn:aws:lambda:us-west-2:123456789012:function:MyFunction</code>.</p> </li> <li> <p> <b>Version or Alias ARN</b> – <code>arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD</code>.</p> </li> <li> <p> <b>Partial ARN</b> – <code>123456789012:function:MyFunction</code>.</p> </li> </ul> <p>The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64 characters in length.</p>"""  
    amazon_managed_kafka_event_source_config: Optional[CreateEventSourceMappingRequestBodyAmazonManagedKafkaEventSourceConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AmazonManagedKafkaEventSourceConfig'), 'exclude': lambda f: f is None }})
    r"""Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source."""  
    batch_size: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('BatchSize'), 'exclude': lambda f: f is None }})
    r"""<p>The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).</p> <ul> <li> <p> <b>Amazon Kinesis</b> – Default 100. Max 10,000.</p> </li> <li> <p> <b>Amazon DynamoDB Streams</b> – Default 100. Max 10,000.</p> </li> <li> <p> <b>Amazon Simple Queue Service</b> – Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.</p> </li> <li> <p> <b>Amazon Managed Streaming for Apache Kafka</b> – Default 100. Max 10,000.</p> </li> <li> <p> <b>Self-managed Apache Kafka</b> – Default 100. Max 10,000.</p> </li> <li> <p> <b>Amazon MQ (ActiveMQ and RabbitMQ)</b> – Default 100. Max 10,000.</p> </li> </ul>"""  
    bisect_batch_on_function_error: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('BisectBatchOnFunctionError'), 'exclude': lambda f: f is None }})
    r"""(Streams only) If the function returns an error, split the batch in two and retry."""  
    destination_config: Optional[CreateEventSourceMappingRequestBodyDestinationConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DestinationConfig'), 'exclude': lambda f: f is None }})
    r"""A configuration object that specifies the destination of an event after Lambda processes it."""  
    document_db_event_source_config: Optional[CreateEventSourceMappingRequestBodyDocumentDBEventSourceConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DocumentDBEventSourceConfig'), 'exclude': lambda f: f is None }})
    r"""Specific configuration settings for a DocumentDB event source."""  
    enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Enabled'), 'exclude': lambda f: f is None }})
    r"""<p>When true, the event source mapping is active. When false, Lambda pauses polling and invocation.</p> <p>Default: True</p>"""  
    event_source_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('EventSourceArn'), 'exclude': lambda f: f is None }})
    r"""<p>The Amazon Resource Name (ARN) of the event source.</p> <ul> <li> <p> <b>Amazon Kinesis</b> – The ARN of the data stream or a stream consumer.</p> </li> <li> <p> <b>Amazon DynamoDB Streams</b> – The ARN of the stream.</p> </li> <li> <p> <b>Amazon Simple Queue Service</b> – The ARN of the queue.</p> </li> <li> <p> <b>Amazon Managed Streaming for Apache Kafka</b> – The ARN of the cluster.</p> </li> <li> <p> <b>Amazon MQ</b> – The ARN of the broker.</p> </li> </ul>"""  
    filter_criteria: Optional[CreateEventSourceMappingRequestBodyFilterCriteria] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FilterCriteria'), 'exclude': lambda f: f is None }})
    r"""An object that contains the filters for an event source."""  
    function_response_types: Optional[list[shared_functionresponsetype_enum.FunctionResponseTypeEnum]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FunctionResponseTypes'), 'exclude': lambda f: f is None }})
    r"""(Streams and Amazon SQS) A list of current response type enums applied to the event source mapping."""  
    maximum_batching_window_in_seconds: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MaximumBatchingWindowInSeconds'), 'exclude': lambda f: f is None }})
    r"""<p>The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function. You can configure <code>MaximumBatchingWindowInSeconds</code> to any value from 0 seconds to 300 seconds in increments of seconds.</p> <p>For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, and Amazon MQ event sources, the default batching window is 500 ms. Note that because you can only change <code>MaximumBatchingWindowInSeconds</code> in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it. To restore the default batching window, you must create a new event source mapping.</p> <p>Related setting: For streams and Amazon SQS event sources, when you set <code>BatchSize</code> to a value greater than 10, you must set <code>MaximumBatchingWindowInSeconds</code> to at least 1.</p>"""  
    maximum_record_age_in_seconds: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MaximumRecordAgeInSeconds'), 'exclude': lambda f: f is None }})
    r"""(Streams only) Discard records older than the specified age. The default value is infinite (-1)."""  
    maximum_retry_attempts: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MaximumRetryAttempts'), 'exclude': lambda f: f is None }})
    r"""(Streams only) Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires."""  
    parallelization_factor: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ParallelizationFactor'), 'exclude': lambda f: f is None }})
    r"""(Streams only) The number of batches to process from each shard concurrently."""  
    queues: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Queues'), 'exclude': lambda f: f is None }})
    r"""(MQ) The name of the Amazon MQ broker destination queue to consume."""  
    scaling_config: Optional[CreateEventSourceMappingRequestBodyScalingConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ScalingConfig'), 'exclude': lambda f: f is None }})
    r"""(Amazon SQS only) The scaling configuration for the event source. To remove the configuration, pass an empty value."""  
    self_managed_event_source: Optional[CreateEventSourceMappingRequestBodySelfManagedEventSource] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('SelfManagedEventSource'), 'exclude': lambda f: f is None }})
    r"""The self-managed Apache Kafka cluster for your event source."""  
    self_managed_kafka_event_source_config: Optional[CreateEventSourceMappingRequestBodySelfManagedKafkaEventSourceConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('SelfManagedKafkaEventSourceConfig'), 'exclude': lambda f: f is None }})
    r"""Specific configuration settings for a self-managed Apache Kafka event source."""  
    source_access_configurations: Optional[list[shared_sourceaccessconfiguration.SourceAccessConfiguration]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('SourceAccessConfigurations'), 'exclude': lambda f: f is None }})
    r"""An array of authentication protocols or VPC components required to secure your event source."""  
    starting_position: Optional[CreateEventSourceMappingRequestBodyStartingPositionEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('StartingPosition'), 'exclude': lambda f: f is None }})
    r"""The position in a stream from which to start reading. Required for Amazon Kinesis, Amazon DynamoDB, and Amazon MSK Streams sources. <code>AT_TIMESTAMP</code> is supported only for Amazon Kinesis streams."""  
    starting_position_timestamp: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('StartingPositionTimestamp'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""With <code>StartingPosition</code> set to <code>AT_TIMESTAMP</code>, the time from which to start reading."""  
    topics: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Topics'), 'exclude': lambda f: f is None }})
    r"""The name of the Kafka topic."""  
    tumbling_window_in_seconds: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('TumblingWindowInSeconds'), 'exclude': lambda f: f is None }})
    r"""(Streams only) The duration in seconds of a processing window. The range is between 1 second and 900 seconds."""  
    

@dataclasses.dataclass
class CreateEventSourceMappingRequest:
    
    request_body: CreateEventSourceMappingRequestBody = dataclasses.field(metadata={'request': { 'media_type': 'application/json' }})  
    x_amz_algorithm: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Algorithm', 'style': 'simple', 'explode': False }})  
    x_amz_content_sha256: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Content-Sha256', 'style': 'simple', 'explode': False }})  
    x_amz_credential: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Credential', 'style': 'simple', 'explode': False }})  
    x_amz_date: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Date', 'style': 'simple', 'explode': False }})  
    x_amz_security_token: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Security-Token', 'style': 'simple', 'explode': False }})  
    x_amz_signature: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Signature', 'style': 'simple', 'explode': False }})  
    x_amz_signed_headers: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-SignedHeaders', 'style': 'simple', 'explode': False }})  
    

@dataclasses.dataclass
class CreateEventSourceMappingResponse:
    
    content_type: str = dataclasses.field()  
    status_code: int = dataclasses.field()  
    event_source_mapping_configuration: Optional[shared_eventsourcemappingconfiguration.EventSourceMappingConfiguration] = dataclasses.field(default=None)
    r"""Success"""  
    invalid_parameter_value_exception: Optional[Any] = dataclasses.field(default=None)
    r"""InvalidParameterValueException"""  
    raw_response: Optional[requests_http.Response] = dataclasses.field(default=None)  
    resource_conflict_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ResourceConflictException"""  
    resource_not_found_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ResourceNotFoundException"""  
    service_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ServiceException"""  
    too_many_requests_exception: Optional[Any] = dataclasses.field(default=None)
    r"""TooManyRequestsException"""  
    