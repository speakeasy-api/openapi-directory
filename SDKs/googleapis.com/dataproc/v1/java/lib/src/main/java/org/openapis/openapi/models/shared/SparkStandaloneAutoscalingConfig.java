/* 
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

package org.openapis.openapi.models.shared;

import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * SparkStandaloneAutoscalingConfig - Basic autoscaling configurations for Spark Standalone.
 */
public class SparkStandaloneAutoscalingConfig {
    /**
     * Required. Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("gracefulDecommissionTimeout")
    public String gracefulDecommissionTimeout;

    public SparkStandaloneAutoscalingConfig withGracefulDecommissionTimeout(String gracefulDecommissionTimeout) {
        this.gracefulDecommissionTimeout = gracefulDecommissionTimeout;
        return this;
    }
    
    /**
     * Required. Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("scaleDownFactor")
    public Double scaleDownFactor;

    public SparkStandaloneAutoscalingConfig withScaleDownFactor(Double scaleDownFactor) {
        this.scaleDownFactor = scaleDownFactor;
        return this;
    }
    
    /**
     * Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("scaleDownMinWorkerFraction")
    public Double scaleDownMinWorkerFraction;

    public SparkStandaloneAutoscalingConfig withScaleDownMinWorkerFraction(Double scaleDownMinWorkerFraction) {
        this.scaleDownMinWorkerFraction = scaleDownMinWorkerFraction;
        return this;
    }
    
    /**
     * Required. Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("scaleUpFactor")
    public Double scaleUpFactor;

    public SparkStandaloneAutoscalingConfig withScaleUpFactor(Double scaleUpFactor) {
        this.scaleUpFactor = scaleUpFactor;
        return this;
    }
    
    /**
     * Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("scaleUpMinWorkerFraction")
    public Double scaleUpMinWorkerFraction;

    public SparkStandaloneAutoscalingConfig withScaleUpMinWorkerFraction(Double scaleUpMinWorkerFraction) {
        this.scaleUpMinWorkerFraction = scaleUpMinWorkerFraction;
        return this;
    }
    
    public SparkStandaloneAutoscalingConfig(){}
}
