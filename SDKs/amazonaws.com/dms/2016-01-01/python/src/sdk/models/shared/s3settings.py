"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import cannedaclforobjectsvalue_enum as shared_cannedaclforobjectsvalue_enum
from ..shared import compressiontypevalue_enum as shared_compressiontypevalue_enum
from ..shared import dataformatvalue_enum as shared_dataformatvalue_enum
from ..shared import datepartitiondelimitervalue_enum as shared_datepartitiondelimitervalue_enum
from ..shared import datepartitionsequencevalue_enum as shared_datepartitionsequencevalue_enum
from ..shared import encodingtypevalue_enum as shared_encodingtypevalue_enum
from ..shared import encryptionmodevalue_enum as shared_encryptionmodevalue_enum
from ..shared import parquetversionvalue_enum as shared_parquetversionvalue_enum
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class S3Settings:
    r"""Settings for exporting data to Amazon S3."""
    
    add_column_name: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AddColumnName'), 'exclude': lambda f: f is None }})  
    add_trailing_padding_character: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AddTrailingPaddingCharacter'), 'exclude': lambda f: f is None }})  
    bucket_folder: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('BucketFolder'), 'exclude': lambda f: f is None }})  
    bucket_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('BucketName'), 'exclude': lambda f: f is None }})  
    canned_acl_for_objects: Optional[shared_cannedaclforobjectsvalue_enum.CannedACLForObjectsValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CannedAclForObjects'), 'exclude': lambda f: f is None }})  
    cdc_inserts_and_updates: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CdcInsertsAndUpdates'), 'exclude': lambda f: f is None }})  
    cdc_inserts_only: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CdcInsertsOnly'), 'exclude': lambda f: f is None }})  
    cdc_max_batch_interval: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CdcMaxBatchInterval'), 'exclude': lambda f: f is None }})  
    cdc_min_file_size: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CdcMinFileSize'), 'exclude': lambda f: f is None }})  
    cdc_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CdcPath'), 'exclude': lambda f: f is None }})  
    compression_type: Optional[shared_compressiontypevalue_enum.CompressionTypeValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CompressionType'), 'exclude': lambda f: f is None }})  
    csv_delimiter: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CsvDelimiter'), 'exclude': lambda f: f is None }})  
    csv_no_sup_value: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CsvNoSupValue'), 'exclude': lambda f: f is None }})  
    csv_null_value: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CsvNullValue'), 'exclude': lambda f: f is None }})  
    csv_row_delimiter: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CsvRowDelimiter'), 'exclude': lambda f: f is None }})  
    data_format: Optional[shared_dataformatvalue_enum.DataFormatValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DataFormat'), 'exclude': lambda f: f is None }})  
    data_page_size: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DataPageSize'), 'exclude': lambda f: f is None }})  
    date_partition_delimiter: Optional[shared_datepartitiondelimitervalue_enum.DatePartitionDelimiterValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DatePartitionDelimiter'), 'exclude': lambda f: f is None }})  
    date_partition_enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DatePartitionEnabled'), 'exclude': lambda f: f is None }})  
    date_partition_sequence: Optional[shared_datepartitionsequencevalue_enum.DatePartitionSequenceValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DatePartitionSequence'), 'exclude': lambda f: f is None }})  
    date_partition_timezone: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DatePartitionTimezone'), 'exclude': lambda f: f is None }})  
    dict_page_size_limit: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DictPageSizeLimit'), 'exclude': lambda f: f is None }})  
    enable_statistics: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('EnableStatistics'), 'exclude': lambda f: f is None }})  
    encoding_type: Optional[shared_encodingtypevalue_enum.EncodingTypeValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('EncodingType'), 'exclude': lambda f: f is None }})  
    encryption_mode: Optional[shared_encryptionmodevalue_enum.EncryptionModeValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('EncryptionMode'), 'exclude': lambda f: f is None }})  
    expected_bucket_owner: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ExpectedBucketOwner'), 'exclude': lambda f: f is None }})  
    external_table_definition: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ExternalTableDefinition'), 'exclude': lambda f: f is None }})  
    glue_catalog_generation: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('GlueCatalogGeneration'), 'exclude': lambda f: f is None }})  
    ignore_header_rows: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('IgnoreHeaderRows'), 'exclude': lambda f: f is None }})  
    include_op_for_full_load: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('IncludeOpForFullLoad'), 'exclude': lambda f: f is None }})  
    max_file_size: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MaxFileSize'), 'exclude': lambda f: f is None }})  
    parquet_timestamp_in_millisecond: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ParquetTimestampInMillisecond'), 'exclude': lambda f: f is None }})  
    parquet_version: Optional[shared_parquetversionvalue_enum.ParquetVersionValueEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ParquetVersion'), 'exclude': lambda f: f is None }})  
    preserve_transactions: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('PreserveTransactions'), 'exclude': lambda f: f is None }})  
    rfc4180: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Rfc4180'), 'exclude': lambda f: f is None }})  
    row_group_length: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('RowGroupLength'), 'exclude': lambda f: f is None }})  
    server_side_encryption_kms_key_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ServerSideEncryptionKmsKeyId'), 'exclude': lambda f: f is None }})  
    service_access_role_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ServiceAccessRoleArn'), 'exclude': lambda f: f is None }})  
    timestamp_column_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('TimestampColumnName'), 'exclude': lambda f: f is None }})  
    use_csv_no_sup_value: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('UseCsvNoSupValue'), 'exclude': lambda f: f is None }})  
    use_task_start_time_for_full_load_timestamp: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('UseTaskStartTimeForFullLoadTimestamp'), 'exclude': lambda f: f is None }})  
    