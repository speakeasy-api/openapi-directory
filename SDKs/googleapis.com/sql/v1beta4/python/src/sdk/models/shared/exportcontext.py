"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ExportContextBakExportOptions:
    r"""Options for exporting BAK files (SQL Server-only)"""
    
    stripe_count: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('stripeCount'), 'exclude': lambda f: f is None }})
    r"""Option for specifying how many stripes to use for the export. If blank, and the value of the striped field is true, the number of stripes is automatically chosen."""  
    striped: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('striped'), 'exclude': lambda f: f is None }})
    r"""Whether or not the export should be striped."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ExportContextCsvExportOptions:
    r"""Options for exporting data as CSV. `MySQL` and `PostgreSQL` instances only."""
    
    escape_character: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('escapeCharacter'), 'exclude': lambda f: f is None }})
    r"""Specifies the character that should appear before a data character that needs to be escaped."""  
    fields_terminated_by: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('fieldsTerminatedBy'), 'exclude': lambda f: f is None }})
    r"""Specifies the character that separates columns within each row (line) of the file."""  
    lines_terminated_by: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('linesTerminatedBy'), 'exclude': lambda f: f is None }})
    r"""This is used to separate lines. If a line does not contain all fields, the rest of the columns are set to their default values."""  
    quote_character: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('quoteCharacter'), 'exclude': lambda f: f is None }})
    r"""Specifies the quoting character to be used when a data value is quoted."""  
    select_query: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('selectQuery'), 'exclude': lambda f: f is None }})
    r"""The select query used to extract the data."""  
    
class ExportContextFileTypeEnum(str, Enum):
    r"""The file type for the specified uri."""
    SQL_FILE_TYPE_UNSPECIFIED = 'SQL_FILE_TYPE_UNSPECIFIED'
    SQL = 'SQL'
    CSV = 'CSV'
    BAK = 'BAK'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ExportContextSQLExportOptionsMysqlExportOptions:
    r"""Options for exporting from MySQL."""
    
    master_data: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('masterData'), 'exclude': lambda f: f is None }})
    r"""Option to include SQL statement required to set up replication. If set to `1`, the dump file includes a CHANGE MASTER TO statement with the binary log coordinates, and --set-gtid-purged is set to ON. If set to `2`, the CHANGE MASTER TO statement is written as a SQL comment and has no effect. If set to any value other than `1`, --set-gtid-purged is set to OFF."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ExportContextSQLExportOptions:
    r"""Options for exporting data as SQL statements."""
    
    mysql_export_options: Optional[ExportContextSQLExportOptionsMysqlExportOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('mysqlExportOptions'), 'exclude': lambda f: f is None }})
    r"""Options for exporting from MySQL."""  
    schema_only: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('schemaOnly'), 'exclude': lambda f: f is None }})
    r"""Export only schemas."""  
    tables: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tables'), 'exclude': lambda f: f is None }})
    r"""Tables to export, or that were exported, from the specified database. If you specify tables, specify one and only one database. For PostgreSQL instances, you can specify only one table."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ExportContext:
    r"""Database instance export context."""
    
    bak_export_options: Optional[ExportContextBakExportOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bakExportOptions'), 'exclude': lambda f: f is None }})
    r"""Options for exporting BAK files (SQL Server-only)"""  
    csv_export_options: Optional[ExportContextCsvExportOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('csvExportOptions'), 'exclude': lambda f: f is None }})
    r"""Options for exporting data as CSV. `MySQL` and `PostgreSQL` instances only."""  
    databases: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('databases'), 'exclude': lambda f: f is None }})
    r"""Databases to be exported. `MySQL instances:` If `fileType` is `SQL` and no database is specified, all databases are exported, except for the `mysql` system database. If `fileType` is `CSV`, you can specify one database, either by using this property or by using the `csvExportOptions.selectQuery` property, which takes precedence over this property. `PostgreSQL instances:` You must specify one database to be exported. If `fileType` is `CSV`, this database must match the one specified in the `csvExportOptions.selectQuery` property. `SQL Server instances:` You must specify one database to be exported, and the `fileType` must be `BAK`."""  
    file_type: Optional[ExportContextFileTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('fileType'), 'exclude': lambda f: f is None }})
    r"""The file type for the specified uri."""  
    kind: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('kind'), 'exclude': lambda f: f is None }})
    r"""This is always `sql#exportContext`."""  
    offload: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('offload'), 'exclude': lambda f: f is None }})
    r"""Option for export offload."""  
    sql_export_options: Optional[ExportContextSQLExportOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sqlExportOptions'), 'exclude': lambda f: f is None }})
    r"""Options for exporting data as SQL statements."""  
    uri: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('uri'), 'exclude': lambda f: f is None }})
    r"""The path to the file in Google Cloud Storage where the export will be stored. The URI is in the form `gs://bucketName/fileName`. If the file already exists, the request succeeds, but the operation fails. If `fileType` is `SQL` and the filename ends with .gz, the contents are compressed."""  
    