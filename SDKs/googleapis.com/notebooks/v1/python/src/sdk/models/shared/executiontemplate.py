"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import dataprocparameters as shared_dataprocparameters
from ..shared import scheduleracceleratorconfig as shared_scheduleracceleratorconfig
from ..shared import vertexaiparameters as shared_vertexaiparameters
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional

class ExecutionTemplateJobTypeEnum(str, Enum):
    r"""The type of Job to be used on this execution."""
    JOB_TYPE_UNSPECIFIED = 'JOB_TYPE_UNSPECIFIED'
    VERTEX_AI = 'VERTEX_AI'
    DATAPROC = 'DATAPROC'

class ExecutionTemplateScaleTierEnum(str, Enum):
    r"""Required. Scale tier of the hardware used for notebook execution. DEPRECATED Will be discontinued. As right now only CUSTOM is supported."""
    SCALE_TIER_UNSPECIFIED = 'SCALE_TIER_UNSPECIFIED'
    BASIC = 'BASIC'
    STANDARD_1 = 'STANDARD_1'
    PREMIUM_1 = 'PREMIUM_1'
    BASIC_GPU = 'BASIC_GPU'
    BASIC_TPU = 'BASIC_TPU'
    CUSTOM = 'CUSTOM'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ExecutionTemplate:
    r"""The description a notebook execution workload."""
    
    accelerator_config: Optional[shared_scheduleracceleratorconfig.SchedulerAcceleratorConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('acceleratorConfig'), 'exclude': lambda f: f is None }})
    r"""Definition of a hardware accelerator. Note that not all combinations of `type` and `core_count` are valid. Check [GPUs on Compute Engine](https://cloud.google.com/compute/docs/gpus) to find a valid combination. TPUs are not supported."""  
    container_image_uri: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('containerImageUri'), 'exclude': lambda f: f is None }})
    r"""Container Image URI to a DLVM Example: 'gcr.io/deeplearning-platform-release/base-cu100' More examples can be found at: https://cloud.google.com/ai-platform/deep-learning-containers/docs/choosing-container"""  
    dataproc_parameters: Optional[shared_dataprocparameters.DataprocParameters] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dataprocParameters'), 'exclude': lambda f: f is None }})
    r"""Parameters used in Dataproc JobType executions."""  
    input_notebook_file: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('inputNotebookFile'), 'exclude': lambda f: f is None }})
    r"""Path to the notebook file to execute. Must be in a Google Cloud Storage bucket. Format: `gs://{bucket_name}/{folder}/{notebook_file_name}` Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook.ipynb`"""  
    job_type: Optional[ExecutionTemplateJobTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobType'), 'exclude': lambda f: f is None }})
    r"""The type of Job to be used on this execution."""  
    kernel_spec: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('kernelSpec'), 'exclude': lambda f: f is None }})
    r"""Name of the kernel spec to use. This must be specified if the kernel spec name on the execution target does not match the name in the input notebook file."""  
    labels: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('labels'), 'exclude': lambda f: f is None }})
    r"""Labels for execution. If execution is scheduled, a field included will be 'nbs-scheduled'. Otherwise, it is an immediate execution, and an included field will be 'nbs-immediate'. Use fields to efficiently index between various types of executions."""  
    master_type: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('masterType'), 'exclude': lambda f: f is None }})
    r"""Specifies the type of virtual machine to use for your training job's master worker. You must specify this field when `scaleTier` is set to `CUSTOM`. You can use certain Compute Engine machine types directly in this field. The following types are supported: - `n1-standard-4` - `n1-standard-8` - `n1-standard-16` - `n1-standard-32` - `n1-standard-64` - `n1-standard-96` - `n1-highmem-2` - `n1-highmem-4` - `n1-highmem-8` - `n1-highmem-16` - `n1-highmem-32` - `n1-highmem-64` - `n1-highmem-96` - `n1-highcpu-16` - `n1-highcpu-32` - `n1-highcpu-64` - `n1-highcpu-96` Alternatively, you can use the following legacy machine types: - `standard` - `large_model` - `complex_model_s` - `complex_model_m` - `complex_model_l` - `standard_gpu` - `complex_model_m_gpu` - `complex_model_l_gpu` - `standard_p100` - `complex_model_m_p100` - `standard_v100` - `large_model_v100` - `complex_model_m_v100` - `complex_model_l_v100` Finally, if you want to use a TPU for training, specify `cloud_tpu` in this field. Learn more about the [special configuration options for training with TPU](https://cloud.google.com/ai-platform/training/docs/using-tpus#configuring_a_custom_tpu_machine)."""  
    output_notebook_folder: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('outputNotebookFolder'), 'exclude': lambda f: f is None }})
    r"""Path to the notebook folder to write to. Must be in a Google Cloud Storage bucket path. Format: `gs://{bucket_name}/{folder}` Ex: `gs://notebook_user/scheduled_notebooks`"""  
    parameters: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('parameters'), 'exclude': lambda f: f is None }})
    r"""Parameters used within the 'input_notebook_file' notebook."""  
    params_yaml_file: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('paramsYamlFile'), 'exclude': lambda f: f is None }})
    r"""Parameters to be overridden in the notebook during execution. Ref https://papermill.readthedocs.io/en/latest/usage-parameterize.html on how to specifying parameters in the input notebook and pass them here in an YAML file. Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook_params.yaml`"""  
    scale_tier: Optional[ExecutionTemplateScaleTierEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('scaleTier'), 'exclude': lambda f: f is None }})
    r"""Required. Scale tier of the hardware used for notebook execution. DEPRECATED Will be discontinued. As right now only CUSTOM is supported."""  
    service_account: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceAccount'), 'exclude': lambda f: f is None }})
    r"""The email address of a service account to use when running the execution. You must have the `iam.serviceAccounts.actAs` permission for the specified service account."""  
    tensorboard: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tensorboard'), 'exclude': lambda f: f is None }})
    r"""The name of a Vertex AI [Tensorboard] resource to which this execution will upload Tensorboard logs. Format: `projects/{project}/locations/{location}/tensorboards/{tensorboard}`"""  
    vertex_ai_parameters: Optional[shared_vertexaiparameters.VertexAIParameters] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('vertexAiParameters'), 'exclude': lambda f: f is None }})
    r"""Parameters used in Vertex AI JobType executions."""  
    