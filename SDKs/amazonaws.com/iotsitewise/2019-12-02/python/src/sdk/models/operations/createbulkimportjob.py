"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import requests as requests_http
from ..shared import createbulkimportjobresponse as shared_createbulkimportjobresponse
from ..shared import file as shared_file
from ..shared import fileformat as shared_fileformat
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Any, Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateBulkImportJobRequestBodyErrorReportLocation:
    r"""The Amazon S3 destination where errors associated with the job creation request are saved."""
    
    bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucket'), 'exclude': lambda f: f is None }})  
    prefix: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('prefix'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateBulkImportJobRequestBodyJobConfiguration:
    r"""Contains the configuration information of a job, such as the file format used to save data in Amazon S3."""
    
    file_format: Optional[shared_fileformat.FileFormat] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('fileFormat'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class CreateBulkImportJobRequestBody:
    
    error_report_location: CreateBulkImportJobRequestBodyErrorReportLocation = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('errorReportLocation') }})
    r"""The Amazon S3 destination where errors associated with the job creation request are saved."""  
    files: list[shared_file.File] = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('files') }})
    r"""The files in the specified Amazon S3 bucket that contain your data."""  
    job_configuration: CreateBulkImportJobRequestBodyJobConfiguration = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobConfiguration') }})
    r"""Contains the configuration information of a job, such as the file format used to save data in Amazon S3."""  
    job_name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobName') }})
    r"""The unique name that helps identify the job request."""  
    job_role_arn: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobRoleArn') }})
    r"""The <a href=\\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\\">ARN</a> of the IAM role that allows IoT SiteWise to read Amazon S3 data."""  
    

@dataclasses.dataclass
class CreateBulkImportJobRequest:
    
    request_body: CreateBulkImportJobRequestBody = dataclasses.field(metadata={'request': { 'media_type': 'application/json' }})  
    x_amz_algorithm: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Algorithm', 'style': 'simple', 'explode': False }})  
    x_amz_content_sha256: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Content-Sha256', 'style': 'simple', 'explode': False }})  
    x_amz_credential: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Credential', 'style': 'simple', 'explode': False }})  
    x_amz_date: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Date', 'style': 'simple', 'explode': False }})  
    x_amz_security_token: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Security-Token', 'style': 'simple', 'explode': False }})  
    x_amz_signature: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Signature', 'style': 'simple', 'explode': False }})  
    x_amz_signed_headers: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-SignedHeaders', 'style': 'simple', 'explode': False }})  
    

@dataclasses.dataclass
class CreateBulkImportJobResponse:
    
    content_type: str = dataclasses.field()  
    status_code: int = dataclasses.field()  
    conflicting_operation_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ConflictingOperationException"""  
    create_bulk_import_job_response: Optional[shared_createbulkimportjobresponse.CreateBulkImportJobResponse] = dataclasses.field(default=None)
    r"""Success"""  
    internal_failure_exception: Optional[Any] = dataclasses.field(default=None)
    r"""InternalFailureException"""  
    invalid_request_exception: Optional[Any] = dataclasses.field(default=None)
    r"""InvalidRequestException"""  
    limit_exceeded_exception: Optional[Any] = dataclasses.field(default=None)
    r"""LimitExceededException"""  
    raw_response: Optional[requests_http.Response] = dataclasses.field(default=None)  
    resource_already_exists_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ResourceAlreadyExistsException"""  
    resource_not_found_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ResourceNotFoundException"""  
    throttling_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ThrottlingException"""  
    