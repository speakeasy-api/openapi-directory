"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import dateutil.parser
from dataclasses_json import Undefined, dataclass_json
from datetime import datetime
from marshmallow import fields
from sdk import utils


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ResourceStatistics:
    r"""Statistics related to resource consumption by compute nodes in a pool."""
    
    avg_cpu_percentage: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('avgCPUPercentage') }})
    r"""The average CPU usage across all nodes in the pool (percentage per node)."""  
    avg_disk_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('avgDiskGiB') }})
    r"""The average used disk space in GiB across all nodes in the pool."""  
    avg_memory_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('avgMemoryGiB') }})
    r"""The average memory usage in GiB across all nodes in the pool."""  
    disk_read_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('diskReadGiB') }})
    r"""The total amount of data in GiB of disk reads across all nodes in the pool."""  
    disk_read_i_ops: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('diskReadIOps') }})
    r"""The total number of disk read operations across all nodes in the pool."""  
    disk_write_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('diskWriteGiB') }})
    r"""The total amount of data in GiB of disk writes across all nodes in the pool."""  
    disk_write_i_ops: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('diskWriteIOps') }})
    r"""The total number of disk write operations across all nodes in the pool."""  
    last_update_time: datetime = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lastUpdateTime'), 'encoder': utils.datetimeisoformat(False), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso') }})
    r"""The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime."""  
    network_read_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('networkReadGiB') }})
    r"""The total amount of data in GiB of network reads across all nodes in the pool."""  
    network_write_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('networkWriteGiB') }})
    r"""The total amount of data in GiB of network writes across all nodes in the pool."""  
    peak_disk_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('peakDiskGiB') }})
    r"""The peak used disk space in GiB across all nodes in the pool."""  
    peak_memory_gi_b: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('peakMemoryGiB') }})
    r"""The peak memory usage in GiB across all nodes in the pool."""  
    start_time: datetime = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('startTime'), 'encoder': utils.datetimeisoformat(False), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso') }})
    r"""The start time of the time range covered by the statistics."""  
    