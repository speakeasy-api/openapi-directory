// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
)

type ImportContextBakImportOptionsEncryptionOptions struct {
	// Path to the Certificate (.cer) in Cloud Storage, in the form `gs://bucketName/fileName`. The instance must have write permissions to the bucket and read access to the file.
	CertPath *string `json:"certPath,omitempty"`
	// Password that encrypts the private key
	PvkPassword *string `json:"pvkPassword,omitempty"`
	// Path to the Certificate Private Key (.pvk) in Cloud Storage, in the form `gs://bucketName/fileName`. The instance must have write permissions to the bucket and read access to the file.
	PvkPath *string `json:"pvkPath,omitempty"`
}

// ImportContextBakImportOptions - Import parameters specific to SQL Server .BAK files
type ImportContextBakImportOptions struct {
	EncryptionOptions *ImportContextBakImportOptionsEncryptionOptions `json:"encryptionOptions,omitempty"`
	// Whether or not the backup set being restored is striped. Applies only to Cloud SQL for SQL Server.
	Striped *bool `json:"striped,omitempty"`
}

// ImportContextCsvImportOptions - Options for importing data as CSV.
type ImportContextCsvImportOptions struct {
	// The columns to which CSV data is imported. If not specified, all columns of the database table are loaded with CSV data.
	Columns []string `json:"columns,omitempty"`
	// Specifies the character that should appear before a data character that needs to be escaped.
	EscapeCharacter *string `json:"escapeCharacter,omitempty"`
	// Specifies the character that separates columns within each row (line) of the file.
	FieldsTerminatedBy *string `json:"fieldsTerminatedBy,omitempty"`
	// This is used to separate lines. If a line does not contain all fields, the rest of the columns are set to their default values.
	LinesTerminatedBy *string `json:"linesTerminatedBy,omitempty"`
	// Specifies the quoting character to be used when a data value is quoted.
	QuoteCharacter *string `json:"quoteCharacter,omitempty"`
	// The table to which CSV data is imported.
	Table *string `json:"table,omitempty"`
}

// ImportContextFileTypeEnum - The file type for the specified uri.\`SQL`: The file contains SQL statements. \`CSV`: The file contains CSV data.
type ImportContextFileTypeEnum string

const (
	ImportContextFileTypeEnumSQLFileTypeUnspecified ImportContextFileTypeEnum = "SQL_FILE_TYPE_UNSPECIFIED"
	ImportContextFileTypeEnumSQL                    ImportContextFileTypeEnum = "SQL"
	ImportContextFileTypeEnumCsv                    ImportContextFileTypeEnum = "CSV"
	ImportContextFileTypeEnumBak                    ImportContextFileTypeEnum = "BAK"
)

func (e ImportContextFileTypeEnum) ToPointer() *ImportContextFileTypeEnum {
	return &e
}

func (e *ImportContextFileTypeEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "SQL_FILE_TYPE_UNSPECIFIED":
		fallthrough
	case "SQL":
		fallthrough
	case "CSV":
		fallthrough
	case "BAK":
		*e = ImportContextFileTypeEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ImportContextFileTypeEnum: %v", v)
	}
}

// ImportContext - Database instance import context.
type ImportContext struct {
	// Import parameters specific to SQL Server .BAK files
	BakImportOptions *ImportContextBakImportOptions `json:"bakImportOptions,omitempty"`
	// Options for importing data as CSV.
	CsvImportOptions *ImportContextCsvImportOptions `json:"csvImportOptions,omitempty"`
	// The target database for the import. If `fileType` is `SQL`, this field is required only if the import file does not specify a database, and is overridden by any database specification in the import file. If `fileType` is `CSV`, one database must be specified.
	Database *string `json:"database,omitempty"`
	// The file type for the specified uri.\`SQL`: The file contains SQL statements. \`CSV`: The file contains CSV data.
	FileType *ImportContextFileTypeEnum `json:"fileType,omitempty"`
	// The PostgreSQL user for this import operation. PostgreSQL instances only.
	ImportUser *string `json:"importUser,omitempty"`
	// This is always `sql#importContext`.
	Kind *string `json:"kind,omitempty"`
	// Path to the import file in Cloud Storage, in the form `gs://bucketName/fileName`. Compressed gzip files (.gz) are supported when `fileType` is `SQL`. The instance must have write permissions to the bucket and read access to the file.
	URI *string `json:"uri,omitempty"`
}
