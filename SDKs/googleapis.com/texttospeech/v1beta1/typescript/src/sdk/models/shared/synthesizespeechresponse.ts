/*
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

import { SpeakeasyBase, SpeakeasyMetadata } from "../../../internal/utils";
import { AudioConfig } from "./audioconfig";
import { Timepoint } from "./timepoint";
import { Expose, Type } from "class-transformer";

/**
 * The message returned to the client by the `SynthesizeSpeech` method.
 */
export class SynthesizeSpeechResponse extends SpeakeasyBase {
  /**
   * Description of audio data to be synthesized.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "audioConfig" })
  @Type(() => AudioConfig)
  audioConfig?: AudioConfig;

  /**
   * The audio data bytes encoded as specified in the request, including the header for encodings that are wrapped in containers (e.g. MP3, OGG_OPUS). For LINEAR16 audio, we include the WAV header. Note: as with all bytes fields, protobuffers use a pure binary representation, whereas JSON representations use base64.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "audioContent" })
  audioContent?: string;

  /**
   * A link between a position in the original request input and a corresponding time in the output audio. It's only supported via `` of SSML input.
   */
  @SpeakeasyMetadata({ elemType: Timepoint })
  @Expose({ name: "timepoints" })
  @Type(() => Timepoint)
  timepoints?: Timepoint[];
}
