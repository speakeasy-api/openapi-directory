// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

// ClassificationEvaluationMetrics - Model evaluation metrics for classification problems. Note: For Video Classification this metrics only describe quality of the Video Classification predictions of "segment_classification" type.
type ClassificationEvaluationMetrics struct {
	// Output only. The annotation spec ids used for this evaluation.
	AnnotationSpecID []string `json:"annotationSpecId,omitempty"`
	// Output only. The Area Under Precision-Recall Curve metric. Micro-averaged for the overall evaluation.
	AuPrc *float32 `json:"auPrc,omitempty"`
	// Output only. The Area Under Receiver Operating Characteristic curve metric. Micro-averaged for the overall evaluation.
	AuRoc *float32 `json:"auRoc,omitempty"`
	// Output only. The Area Under Precision-Recall Curve metric based on priors. Micro-averaged for the overall evaluation. Deprecated.
	BaseAuPrc *float32 `json:"baseAuPrc,omitempty"`
	// Output only. Metrics for each confidence_threshold in 0.00,0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 and position_threshold = INT32_MAX_VALUE. ROC and precision-recall curves, and other aggregated metrics are derived from them. The confidence metrics entries may also be supplied for additional values of position_threshold, but from these no aggregated metrics are computed.
	ConfidenceMetricsEntry []ClassificationEvaluationMetricsConfidenceMetricsEntry `json:"confidenceMetricsEntry,omitempty"`
	// Confusion matrix of the model running the classification.
	ConfusionMatrix *ConfusionMatrix `json:"confusionMatrix,omitempty"`
	// Output only. The Log Loss metric.
	LogLoss *float32 `json:"logLoss,omitempty"`
}
