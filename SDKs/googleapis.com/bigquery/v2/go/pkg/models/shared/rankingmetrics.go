// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

// RankingMetrics - Evaluation metrics used by weighted-ALS models specified by feedback_type=implicit.
type RankingMetrics struct {
	// Determines the goodness of a ranking by computing the percentile rank from the predicted confidence and dividing it by the original rank.
	AverageRank *float64 `json:"averageRank,omitempty"`
	// Calculates a precision per user for all the items by ranking them and then averages all the precisions across all the users.
	MeanAveragePrecision *float64 `json:"meanAveragePrecision,omitempty"`
	// Similar to the mean squared error computed in regression and explicit recommendation models except instead of computing the rating directly, the output from evaluate is computed against a preference which is 1 or 0 depending on if the rating exists or not.
	MeanSquaredError *float64 `json:"meanSquaredError,omitempty"`
	// A metric to determine the goodness of a ranking calculated from the predicted confidence by comparing it to an ideal rank measured by the original ratings.
	NormalizedDiscountedCumulativeGain *float64 `json:"normalizedDiscountedCumulativeGain,omitempty"`
}
