"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import requests as requests_http
from ..shared import moduleloggingconfigurationinput as shared_moduleloggingconfigurationinput
from ..shared import updateenvironmentoutput as shared_updateenvironmentoutput
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Any, Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class UpdateEnvironmentRequestBodyLoggingConfiguration:
    r"""Defines the Apache Airflow log types to send to CloudWatch Logs."""
    
    dag_processing_logs: Optional[shared_moduleloggingconfigurationinput.ModuleLoggingConfigurationInput] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DagProcessingLogs'), 'exclude': lambda f: f is None }})  
    scheduler_logs: Optional[shared_moduleloggingconfigurationinput.ModuleLoggingConfigurationInput] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('SchedulerLogs'), 'exclude': lambda f: f is None }})  
    task_logs: Optional[shared_moduleloggingconfigurationinput.ModuleLoggingConfigurationInput] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('TaskLogs'), 'exclude': lambda f: f is None }})  
    webserver_logs: Optional[shared_moduleloggingconfigurationinput.ModuleLoggingConfigurationInput] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('WebserverLogs'), 'exclude': lambda f: f is None }})  
    worker_logs: Optional[shared_moduleloggingconfigurationinput.ModuleLoggingConfigurationInput] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('WorkerLogs'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class UpdateEnvironmentRequestBodyNetworkConfiguration:
    r"""Defines the VPC networking components used to secure and enable network traffic between the Amazon Web Services resources for your environment. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/networking-about.html\\">About networking on Amazon MWAA</a>."""
    
    security_group_ids: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('SecurityGroupIds'), 'exclude': lambda f: f is None }})  
    
class UpdateEnvironmentRequestBodyWebserverAccessModeEnum(str, Enum):
    r"""The Apache Airflow <i>Web server</i> access mode. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-networking.html\\">Apache Airflow access modes</a>."""
    PRIVATE_ONLY = 'PRIVATE_ONLY'
    PUBLIC_ONLY = 'PUBLIC_ONLY'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class UpdateEnvironmentRequestBody:
    
    airflow_configuration_options: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AirflowConfigurationOptions'), 'exclude': lambda f: f is None }})
    r"""A list of key-value pairs containing the Apache Airflow configuration options you want to attach to your environment. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-env-variables.html\\">Apache Airflow configuration options</a>."""  
    airflow_version: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AirflowVersion'), 'exclude': lambda f: f is None }})
    r"""The Apache Airflow version for your environment. If no value is specified, defaults to the latest version. Valid values: <code>1.10.12</code>, <code>2.0.2</code>, <code>2.2.2</code>, and <code>2.4.3</code>."""  
    dag_s3_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DagS3Path'), 'exclude': lambda f: f is None }})
    r"""The relative path to the DAGs folder on your Amazon S3 bucket. For example, <code>dags</code>. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-dag-folder.html\\">Adding or updating DAGs</a>."""  
    environment_class: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('EnvironmentClass'), 'exclude': lambda f: f is None }})
    r"""The environment class type. Valid values: <code>mw1.small</code>, <code>mw1.medium</code>, <code>mw1.large</code>. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/environment-class.html\\">Amazon MWAA environment class</a>."""  
    execution_role_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ExecutionRoleArn'), 'exclude': lambda f: f is None }})
    r"""The Amazon Resource Name (ARN) of the execution role in IAM that allows MWAA to access Amazon Web Services resources in your environment. For example, <code>arn:aws:iam::123456789:role/my-execution-role</code>. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/mwaa-create-role.html\\">Amazon MWAA Execution role</a>."""  
    logging_configuration: Optional[UpdateEnvironmentRequestBodyLoggingConfiguration] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('LoggingConfiguration'), 'exclude': lambda f: f is None }})
    r"""Defines the Apache Airflow log types to send to CloudWatch Logs."""  
    max_workers: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MaxWorkers'), 'exclude': lambda f: f is None }})
    r"""The maximum number of workers that you want to run in your environment. MWAA scales the number of Apache Airflow workers up to the number you specify in the <code>MaxWorkers</code> field. For example, <code>20</code>. When there are no more tasks running, and no more in the queue, MWAA disposes of the extra workers leaving the one worker that is included with your environment, or the number you specify in <code>MinWorkers</code>."""  
    min_workers: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('MinWorkers'), 'exclude': lambda f: f is None }})
    r"""The minimum number of workers that you want to run in your environment. MWAA scales the number of Apache Airflow workers up to the number you specify in the <code>MaxWorkers</code> field. When there are no more tasks running, and no more in the queue, MWAA disposes of the extra workers leaving the worker count you specify in the <code>MinWorkers</code> field. For example, <code>2</code>."""  
    network_configuration: Optional[UpdateEnvironmentRequestBodyNetworkConfiguration] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('NetworkConfiguration'), 'exclude': lambda f: f is None }})
    r"""Defines the VPC networking components used to secure and enable network traffic between the Amazon Web Services resources for your environment. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/networking-about.html\\">About networking on Amazon MWAA</a>."""  
    plugins_s3_object_version: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('PluginsS3ObjectVersion'), 'exclude': lambda f: f is None }})
    r"""The version of the plugins.zip file on your Amazon S3 bucket. A version must be specified each time a plugins.zip file is updated. To learn more, see <a href=\\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html\\">How S3 Versioning works</a>."""  
    plugins_s3_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('PluginsS3Path'), 'exclude': lambda f: f is None }})
    r"""The relative path to the <code>plugins.zip</code> file on your Amazon S3 bucket. For example, <code>plugins.zip</code>. If specified, then the plugins.zip version is required. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-dag-import-plugins.html\\">Installing custom plugins</a>."""  
    requirements_s3_object_version: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('RequirementsS3ObjectVersion'), 'exclude': lambda f: f is None }})
    r"""The version of the requirements.txt file on your Amazon S3 bucket. A version must be specified each time a requirements.txt file is updated. To learn more, see <a href=\\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html\\">How S3 Versioning works</a>."""  
    requirements_s3_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('RequirementsS3Path'), 'exclude': lambda f: f is None }})
    r"""The relative path to the <code>requirements.txt</code> file on your Amazon S3 bucket. For example, <code>requirements.txt</code>. If specified, then a file version is required. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/working-dags-dependencies.html\\">Installing Python dependencies</a>."""  
    schedulers: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Schedulers'), 'exclude': lambda f: f is None }})
    r"""The number of Apache Airflow schedulers to run in your Amazon MWAA environment."""  
    source_bucket_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('SourceBucketArn'), 'exclude': lambda f: f is None }})
    r"""The Amazon Resource Name (ARN) of the Amazon S3 bucket where your DAG code and supporting files are stored. For example, <code>arn:aws:s3:::my-airflow-bucket-unique-name</code>. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/mwaa-s3-bucket.html\\">Create an Amazon S3 bucket for Amazon MWAA</a>."""  
    webserver_access_mode: Optional[UpdateEnvironmentRequestBodyWebserverAccessModeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('WebserverAccessMode'), 'exclude': lambda f: f is None }})
    r"""The Apache Airflow <i>Web server</i> access mode. To learn more, see <a href=\\"https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-networking.html\\">Apache Airflow access modes</a>."""  
    weekly_maintenance_window_start: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('WeeklyMaintenanceWindowStart'), 'exclude': lambda f: f is None }})
    r"""The day and time of the week in Coordinated Universal Time (UTC) 24-hour standard time to start weekly maintenance updates of your environment in the following format: <code>DAY:HH:MM</code>. For example: <code>TUE:03:30</code>. You can specify a start time in 30 minute increments only."""  
    

@dataclasses.dataclass
class UpdateEnvironmentRequest:
    
    name: str = dataclasses.field(metadata={'path_param': { 'field_name': 'Name', 'style': 'simple', 'explode': False }})
    r"""The name of your Amazon MWAA environment. For example, <code>MyMWAAEnvironment</code>."""  
    request_body: UpdateEnvironmentRequestBody = dataclasses.field(metadata={'request': { 'media_type': 'application/json' }})  
    x_amz_algorithm: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Algorithm', 'style': 'simple', 'explode': False }})  
    x_amz_content_sha256: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Content-Sha256', 'style': 'simple', 'explode': False }})  
    x_amz_credential: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Credential', 'style': 'simple', 'explode': False }})  
    x_amz_date: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Date', 'style': 'simple', 'explode': False }})  
    x_amz_security_token: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Security-Token', 'style': 'simple', 'explode': False }})  
    x_amz_signature: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Signature', 'style': 'simple', 'explode': False }})  
    x_amz_signed_headers: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-SignedHeaders', 'style': 'simple', 'explode': False }})  
    

@dataclasses.dataclass
class UpdateEnvironmentResponse:
    
    content_type: str = dataclasses.field()  
    status_code: int = dataclasses.field()  
    internal_server_exception: Optional[Any] = dataclasses.field(default=None)
    r"""InternalServerException"""  
    raw_response: Optional[requests_http.Response] = dataclasses.field(default=None)  
    resource_not_found_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ResourceNotFoundException"""  
    update_environment_output: Optional[shared_updateenvironmentoutput.UpdateEnvironmentOutput] = dataclasses.field(default=None)
    r"""Success"""  
    validation_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ValidationException"""  
    