"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ImportContextBakImportOptionsEncryptionOptions:
    
    cert_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('certPath'), 'exclude': lambda f: f is None }})
    r"""Path to the Certificate (.cer) in Cloud Storage, in the form `gs://bucketName/fileName`. The instance must have write permissions to the bucket and read access to the file."""  
    pvk_password: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pvkPassword'), 'exclude': lambda f: f is None }})
    r"""Password that encrypts the private key"""  
    pvk_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pvkPath'), 'exclude': lambda f: f is None }})
    r"""Path to the Certificate Private Key (.pvk) in Cloud Storage, in the form `gs://bucketName/fileName`. The instance must have write permissions to the bucket and read access to the file."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ImportContextBakImportOptions:
    r"""Import parameters specific to SQL Server .BAK files"""
    
    encryption_options: Optional[ImportContextBakImportOptionsEncryptionOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('encryptionOptions'), 'exclude': lambda f: f is None }})  
    striped: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('striped'), 'exclude': lambda f: f is None }})
    r"""Whether or not the backup set being restored is striped. Applies only to Cloud SQL for SQL Server."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ImportContextCsvImportOptions:
    r"""Options for importing data as CSV."""
    
    columns: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('columns'), 'exclude': lambda f: f is None }})
    r"""The columns to which CSV data is imported. If not specified, all columns of the database table are loaded with CSV data."""  
    escape_character: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('escapeCharacter'), 'exclude': lambda f: f is None }})
    r"""Specifies the character that should appear before a data character that needs to be escaped."""  
    fields_terminated_by: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('fieldsTerminatedBy'), 'exclude': lambda f: f is None }})
    r"""Specifies the character that separates columns within each row (line) of the file."""  
    lines_terminated_by: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('linesTerminatedBy'), 'exclude': lambda f: f is None }})
    r"""This is used to separate lines. If a line does not contain all fields, the rest of the columns are set to their default values."""  
    quote_character: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('quoteCharacter'), 'exclude': lambda f: f is None }})
    r"""Specifies the quoting character to be used when a data value is quoted."""  
    table: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('table'), 'exclude': lambda f: f is None }})
    r"""The table to which CSV data is imported."""  
    
class ImportContextFileTypeEnum(str, Enum):
    r"""The file type for the specified uri.\`SQL`: The file contains SQL statements. \`CSV`: The file contains CSV data."""
    SQL_FILE_TYPE_UNSPECIFIED = 'SQL_FILE_TYPE_UNSPECIFIED'
    SQL = 'SQL'
    CSV = 'CSV'
    BAK = 'BAK'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ImportContext:
    r"""Database instance import context."""
    
    bak_import_options: Optional[ImportContextBakImportOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bakImportOptions'), 'exclude': lambda f: f is None }})
    r"""Import parameters specific to SQL Server .BAK files"""  
    csv_import_options: Optional[ImportContextCsvImportOptions] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('csvImportOptions'), 'exclude': lambda f: f is None }})
    r"""Options for importing data as CSV."""  
    database: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('database'), 'exclude': lambda f: f is None }})
    r"""The target database for the import. If `fileType` is `SQL`, this field is required only if the import file does not specify a database, and is overridden by any database specification in the import file. If `fileType` is `CSV`, one database must be specified."""  
    file_type: Optional[ImportContextFileTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('fileType'), 'exclude': lambda f: f is None }})
    r"""The file type for the specified uri.\`SQL`: The file contains SQL statements. \`CSV`: The file contains CSV data."""  
    import_user: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('importUser'), 'exclude': lambda f: f is None }})
    r"""The PostgreSQL user for this import operation. PostgreSQL instances only."""  
    kind: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('kind'), 'exclude': lambda f: f is None }})
    r"""This is always `sql#importContext`."""  
    uri: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('uri'), 'exclude': lambda f: f is None }})
    r"""Path to the import file in Cloud Storage, in the form `gs://bucketName/fileName`. Compressed gzip files (.gz) are supported when `fileType` is `SQL`. The instance must have write permissions to the bucket and read access to the file."""  
    