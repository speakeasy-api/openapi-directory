"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import requests as requests_http
from ..shared import configuration as shared_configuration
from ..shared import monitoringconfiguration as shared_monitoringconfiguration
from ..shared import sparksqljobdriver as shared_sparksqljobdriver
from ..shared import sparksubmitjobdriver as shared_sparksubmitjobdriver
from ..shared import startjobrunresponse as shared_startjobrunresponse
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Any, Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class StartJobRunRequestBodyConfigurationOverrides:
    r"""A configuration specification to be used to override existing configurations."""
    
    application_configuration: Optional[list[shared_configuration.Configuration]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('applicationConfiguration'), 'exclude': lambda f: f is None }})  
    monitoring_configuration: Optional[shared_monitoringconfiguration.MonitoringConfiguration] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('monitoringConfiguration'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class StartJobRunRequestBodyJobDriver:
    r"""Specify the driver that the job runs on. Exactly one of the two available job drivers is required, either sparkSqlJobDriver or sparkSubmitJobDriver."""
    
    spark_sql_job_driver: Optional[shared_sparksqljobdriver.SparkSQLJobDriver] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkSqlJobDriver'), 'exclude': lambda f: f is None }})  
    spark_submit_job_driver: Optional[shared_sparksubmitjobdriver.SparkSubmitJobDriver] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkSubmitJobDriver'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class StartJobRunRequestBodyRetryPolicyConfiguration:
    r"""The configuration of the retry policy that the job runs on."""
    
    max_attempts: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('maxAttempts'), 'exclude': lambda f: f is None }})  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class StartJobRunRequestBody:
    
    client_token: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('clientToken') }})
    r"""The client idempotency token of the job run request."""  
    configuration_overrides: Optional[StartJobRunRequestBodyConfigurationOverrides] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('configurationOverrides'), 'exclude': lambda f: f is None }})
    r"""A configuration specification to be used to override existing configurations."""  
    execution_role_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('executionRoleArn'), 'exclude': lambda f: f is None }})
    r"""The execution role ARN for the job run."""  
    job_driver: Optional[StartJobRunRequestBodyJobDriver] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobDriver'), 'exclude': lambda f: f is None }})
    r"""Specify the driver that the job runs on. Exactly one of the two available job drivers is required, either sparkSqlJobDriver or sparkSubmitJobDriver."""  
    job_template_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobTemplateId'), 'exclude': lambda f: f is None }})
    r"""The job template ID to be used to start the job run."""  
    job_template_parameters: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('jobTemplateParameters'), 'exclude': lambda f: f is None }})
    r"""The values of job template parameters to start a job run."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""The name of the job run."""  
    release_label: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('releaseLabel'), 'exclude': lambda f: f is None }})
    r"""The Amazon EMR release version to use for the job run."""  
    retry_policy_configuration: Optional[StartJobRunRequestBodyRetryPolicyConfiguration] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('retryPolicyConfiguration'), 'exclude': lambda f: f is None }})
    r"""The configuration of the retry policy that the job runs on."""  
    tags: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tags'), 'exclude': lambda f: f is None }})
    r"""The tags assigned to job runs."""  
    

@dataclasses.dataclass
class StartJobRunRequest:
    
    request_body: StartJobRunRequestBody = dataclasses.field(metadata={'request': { 'media_type': 'application/json' }})  
    virtual_cluster_id: str = dataclasses.field(metadata={'path_param': { 'field_name': 'virtualClusterId', 'style': 'simple', 'explode': False }})
    r"""The virtual cluster ID for which the job run request is submitted."""  
    x_amz_algorithm: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Algorithm', 'style': 'simple', 'explode': False }})  
    x_amz_content_sha256: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Content-Sha256', 'style': 'simple', 'explode': False }})  
    x_amz_credential: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Credential', 'style': 'simple', 'explode': False }})  
    x_amz_date: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Date', 'style': 'simple', 'explode': False }})  
    x_amz_security_token: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Security-Token', 'style': 'simple', 'explode': False }})  
    x_amz_signature: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-Signature', 'style': 'simple', 'explode': False }})  
    x_amz_signed_headers: Optional[str] = dataclasses.field(default=None, metadata={'header': { 'field_name': 'X-Amz-SignedHeaders', 'style': 'simple', 'explode': False }})  
    

@dataclasses.dataclass
class StartJobRunResponse:
    
    content_type: str = dataclasses.field()  
    status_code: int = dataclasses.field()  
    internal_server_exception: Optional[Any] = dataclasses.field(default=None)
    r"""InternalServerException"""  
    raw_response: Optional[requests_http.Response] = dataclasses.field(default=None)  
    resource_not_found_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ResourceNotFoundException"""  
    start_job_run_response: Optional[shared_startjobrunresponse.StartJobRunResponse] = dataclasses.field(default=None)
    r"""Success"""  
    validation_exception: Optional[Any] = dataclasses.field(default=None)
    r"""ValidationException"""  
    