"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import dateutil.parser
from ..shared import bucketaccesscontrol as shared_bucketaccesscontrol
from ..shared import objectaccesscontrol as shared_objectaccesscontrol
from dataclasses_json import Undefined, dataclass_json
from datetime import date, datetime
from marshmallow import fields
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketAutoclass:
    r"""The bucket's Autoclass configuration."""
    
    enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('enabled'), 'exclude': lambda f: f is None }})
    r"""Whether or not Autoclass is enabled on this bucket"""  
    toggle_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('toggleTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""A date and time in RFC 3339 format representing the instant at which \\"enabled\\" was last toggled."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketBilling:
    r"""The bucket's billing configuration."""
    
    requester_pays: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('requesterPays'), 'exclude': lambda f: f is None }})
    r"""When set to true, Requester Pays is enabled for this bucket."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketCors:
    
    max_age_seconds: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('maxAgeSeconds'), 'exclude': lambda f: f is None }})
    r"""The value, in seconds, to return in the  Access-Control-Max-Age header used in preflight responses."""  
    method: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('method'), 'exclude': lambda f: f is None }})
    r"""The list of HTTP methods on which to include CORS response headers, (GET, OPTIONS, POST, etc) Note: \\"*\\" is permitted in the list of methods, and means \\"any method\\"."""  
    origin: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('origin'), 'exclude': lambda f: f is None }})
    r"""The list of Origins eligible to receive CORS response headers. Note: \\"*\\" is permitted in the list of origins, and means \\"any Origin\\"."""  
    response_header: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('responseHeader'), 'exclude': lambda f: f is None }})
    r"""The list of HTTP headers other than the simple response headers to give permission for the user-agent to share across domains."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketCustomPlacementConfig:
    r"""The bucket's custom placement configuration for Custom Dual Regions."""
    
    data_locations: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dataLocations'), 'exclude': lambda f: f is None }})
    r"""The list of regional locations in which data is placed."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketEncryption:
    r"""Encryption configuration for a bucket."""
    
    default_kms_key_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('defaultKmsKeyName'), 'exclude': lambda f: f is None }})
    r"""A Cloud KMS key that will be used to encrypt objects inserted into this bucket, if no encryption method is specified."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketIamConfigurationBucketPolicyOnly:
    r"""The bucket's uniform bucket-level access configuration. The feature was formerly known as Bucket Policy Only. For backward compatibility, this field will be populated with identical information as the uniformBucketLevelAccess field. We recommend using the uniformBucketLevelAccess field to enable and disable the feature."""
    
    enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('enabled'), 'exclude': lambda f: f is None }})
    r"""If set, access is controlled only by bucket-level or above IAM policies."""  
    locked_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lockedTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""The deadline for changing iamConfiguration.bucketPolicyOnly.enabled from true to false in RFC 3339 format. iamConfiguration.bucketPolicyOnly.enabled may be changed from true to false until the locked time, after which the field is immutable."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketIamConfigurationUniformBucketLevelAccess:
    r"""The bucket's uniform bucket-level access configuration."""
    
    enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('enabled'), 'exclude': lambda f: f is None }})
    r"""If set, access is controlled only by bucket-level or above IAM policies."""  
    locked_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lockedTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""The deadline for changing iamConfiguration.uniformBucketLevelAccess.enabled from true to false in RFC 3339  format. iamConfiguration.uniformBucketLevelAccess.enabled may be changed from true to false until the locked time, after which the field is immutable."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketIamConfiguration:
    r"""The bucket's IAM configuration."""
    
    bucket_policy_only: Optional[BucketIamConfigurationBucketPolicyOnly] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucketPolicyOnly'), 'exclude': lambda f: f is None }})
    r"""The bucket's uniform bucket-level access configuration. The feature was formerly known as Bucket Policy Only. For backward compatibility, this field will be populated with identical information as the uniformBucketLevelAccess field. We recommend using the uniformBucketLevelAccess field to enable and disable the feature."""  
    public_access_prevention: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('publicAccessPrevention'), 'exclude': lambda f: f is None }})
    r"""The bucket's Public Access Prevention configuration. Currently, 'inherited' and 'enforced' are supported."""  
    uniform_bucket_level_access: Optional[BucketIamConfigurationUniformBucketLevelAccess] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('uniformBucketLevelAccess'), 'exclude': lambda f: f is None }})
    r"""The bucket's uniform bucket-level access configuration."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketLifecycleRuleAction:
    r"""The action to take."""
    
    storage_class: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('storageClass'), 'exclude': lambda f: f is None }})
    r"""Target storage class. Required iff the type of the action is SetStorageClass."""  
    type: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""Type of the action. Currently, only Delete, SetStorageClass, and AbortIncompleteMultipartUpload are supported."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketLifecycleRuleCondition:
    r"""The condition(s) under which the action will be taken."""
    
    age: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('age'), 'exclude': lambda f: f is None }})
    r"""Age of an object (in days). This condition is satisfied when an object reaches the specified age."""  
    created_before: Optional[date] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('createdBefore'), 'encoder': utils.dateisoformat(True), 'decoder': utils.datefromisoformat, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""A date in RFC 3339 format with only the date part (for instance, \\"2013-01-15\\"). This condition is satisfied when an object is created before midnight of the specified date in UTC."""  
    custom_time_before: Optional[date] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('customTimeBefore'), 'encoder': utils.dateisoformat(True), 'decoder': utils.datefromisoformat, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""A date in RFC 3339 format with only the date part (for instance, \\"2013-01-15\\"). This condition is satisfied when the custom time on an object is before this date in UTC."""  
    days_since_custom_time: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('daysSinceCustomTime'), 'exclude': lambda f: f is None }})
    r"""Number of days elapsed since the user-specified timestamp set on an object. The condition is satisfied if the days elapsed is at least this number. If no custom timestamp is specified on an object, the condition does not apply."""  
    days_since_noncurrent_time: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('daysSinceNoncurrentTime'), 'exclude': lambda f: f is None }})
    r"""Number of days elapsed since the noncurrent timestamp of an object. The condition is satisfied if the days elapsed is at least this number. This condition is relevant only for versioned objects. The value of the field must be a nonnegative integer. If it's zero, the object version will become eligible for Lifecycle action as soon as it becomes noncurrent."""  
    is_live: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('isLive'), 'exclude': lambda f: f is None }})
    r"""Relevant only for versioned objects. If the value is true, this condition matches live objects; if the value is false, it matches archived objects."""  
    matches_pattern: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('matchesPattern'), 'exclude': lambda f: f is None }})
    r"""A regular expression that satisfies the RE2 syntax. This condition is satisfied when the name of the object matches the RE2 pattern. Note: This feature is currently in the \\"Early Access\\" launch stage and is only available to a whitelisted set of users; that means that this feature may be changed in backward-incompatible ways and that it is not guaranteed to be released."""  
    matches_prefix: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('matchesPrefix'), 'exclude': lambda f: f is None }})
    r"""List of object name prefixes. This condition will be satisfied when at least one of the prefixes exactly matches the beginning of the object name."""  
    matches_storage_class: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('matchesStorageClass'), 'exclude': lambda f: f is None }})
    r"""Objects having any of the storage classes specified by this condition will be matched. Values include MULTI_REGIONAL, REGIONAL, NEARLINE, COLDLINE, ARCHIVE, STANDARD, and DURABLE_REDUCED_AVAILABILITY."""  
    matches_suffix: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('matchesSuffix'), 'exclude': lambda f: f is None }})
    r"""List of object name suffixes. This condition will be satisfied when at least one of the suffixes exactly matches the end of the object name."""  
    noncurrent_time_before: Optional[date] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('noncurrentTimeBefore'), 'encoder': utils.dateisoformat(True), 'decoder': utils.datefromisoformat, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""A date in RFC 3339 format with only the date part (for instance, \\"2013-01-15\\"). This condition is satisfied when the noncurrent time on an object is before this date in UTC. This condition is relevant only for versioned objects."""  
    num_newer_versions: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('numNewerVersions'), 'exclude': lambda f: f is None }})
    r"""Relevant only for versioned objects. If the value is N, this condition is satisfied when there are at least N versions (including the live version) newer than this version of the object."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketLifecycleRule:
    
    action: Optional[BucketLifecycleRuleAction] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('action'), 'exclude': lambda f: f is None }})
    r"""The action to take."""  
    condition: Optional[BucketLifecycleRuleCondition] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('condition'), 'exclude': lambda f: f is None }})
    r"""The condition(s) under which the action will be taken."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketLifecycle:
    r"""The bucket's lifecycle configuration. See lifecycle management for more information."""
    
    rule: Optional[list[BucketLifecycleRule]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('rule'), 'exclude': lambda f: f is None }})
    r"""A lifecycle management rule, which is made of an action to take and the condition(s) under which the action will be taken."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketLogging:
    r"""The bucket's logging configuration, which defines the destination bucket and optional name prefix for the current bucket's logs."""
    
    log_bucket: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logBucket'), 'exclude': lambda f: f is None }})
    r"""The destination bucket where the current bucket's logs should be placed."""  
    log_object_prefix: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logObjectPrefix'), 'exclude': lambda f: f is None }})
    r"""A prefix for log object names."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketOwner:
    r"""The owner of the bucket. This is always the project team's owner group."""
    
    entity: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('entity'), 'exclude': lambda f: f is None }})
    r"""The entity, in the form project-owner-projectId."""  
    entity_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('entityId'), 'exclude': lambda f: f is None }})
    r"""The ID for the entity."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketRetentionPolicy:
    r"""The bucket's retention policy. The retention policy enforces a minimum retention time for all objects contained in the bucket, based on their creation time. Any attempt to overwrite or delete objects younger than the retention period will result in a PERMISSION_DENIED error. An unlocked retention policy can be modified or removed from the bucket via a storage.buckets.update operation. A locked retention policy cannot be removed or shortened in duration for the lifetime of the bucket. Attempting to remove or decrease period of a locked retention policy will result in a PERMISSION_DENIED error."""
    
    effective_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('effectiveTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""Server-determined value that indicates the time from which policy was enforced and effective. This value is in RFC 3339 format."""  
    is_locked: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('isLocked'), 'exclude': lambda f: f is None }})
    r"""Once locked, an object retention policy cannot be modified."""  
    retention_period: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('retentionPeriod'), 'exclude': lambda f: f is None }})
    r"""The duration in seconds that objects need to be retained. Retention duration must be greater than zero and less than 100 years. Note that enforcement of retention periods less than a day is not guaranteed. Such periods should only be used for testing purposes."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketVersioning:
    r"""The bucket's versioning configuration."""
    
    enabled: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('enabled'), 'exclude': lambda f: f is None }})
    r"""While set to true, versioning is fully enabled for this bucket."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class BucketWebsite:
    r"""The bucket's website configuration, controlling how the service behaves when accessing bucket contents as a web site. See the Static Website Examples for more information."""
    
    main_page_suffix: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('mainPageSuffix'), 'exclude': lambda f: f is None }})
    r"""If the requested object path is missing, the service will ensure the path has a trailing '/', append this suffix, and attempt to retrieve the resulting object. This allows the creation of index.html objects to represent directory pages."""  
    not_found_page: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('notFoundPage'), 'exclude': lambda f: f is None }})
    r"""If the requested object path is missing, and any mainPageSuffix object is missing, if applicable, the service will return the named object from this bucket as the content for a 404 Not Found result."""  
    

@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class Bucket:
    r"""A bucket."""
    
    acl: Optional[list[shared_bucketaccesscontrol.BucketAccessControl]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('acl'), 'exclude': lambda f: f is None }})
    r"""Access controls on the bucket."""  
    autoclass: Optional[BucketAutoclass] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('autoclass'), 'exclude': lambda f: f is None }})
    r"""The bucket's Autoclass configuration."""  
    billing: Optional[BucketBilling] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('billing'), 'exclude': lambda f: f is None }})
    r"""The bucket's billing configuration."""  
    cors: Optional[list[BucketCors]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('cors'), 'exclude': lambda f: f is None }})
    r"""The bucket's Cross-Origin Resource Sharing (CORS) configuration."""  
    custom_placement_config: Optional[BucketCustomPlacementConfig] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('customPlacementConfig'), 'exclude': lambda f: f is None }})
    r"""The bucket's custom placement configuration for Custom Dual Regions."""  
    default_event_based_hold: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('defaultEventBasedHold'), 'exclude': lambda f: f is None }})
    r"""The default value for event-based hold on newly created objects in this bucket. Event-based hold is a way to retain objects indefinitely until an event occurs, signified by the hold's release. After being released, such objects will be subject to bucket-level retention (if any). One sample use case of this flag is for banks to hold loan documents for at least 3 years after loan is paid in full. Here, bucket-level retention is 3 years and the event is loan being paid in full. In this example, these objects will be held intact for any number of years until the event has occurred (event-based hold on the object is released) and then 3 more years after that. That means retention duration of the objects begins from the moment event-based hold transitioned from true to false. Objects under event-based hold cannot be deleted, overwritten or archived until the hold is removed."""  
    default_object_acl: Optional[list[shared_objectaccesscontrol.ObjectAccessControl]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('defaultObjectAcl'), 'exclude': lambda f: f is None }})
    r"""Default access controls to apply to new objects when no ACL is provided."""  
    encryption: Optional[BucketEncryption] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('encryption'), 'exclude': lambda f: f is None }})
    r"""Encryption configuration for a bucket."""  
    etag: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('etag'), 'exclude': lambda f: f is None }})
    r"""HTTP 1.1 Entity tag for the bucket."""  
    iam_configuration: Optional[BucketIamConfiguration] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('iamConfiguration'), 'exclude': lambda f: f is None }})
    r"""The bucket's IAM configuration."""  
    id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('id'), 'exclude': lambda f: f is None }})
    r"""The ID of the bucket. For buckets, the id and name properties are the same."""  
    kind: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('kind'), 'exclude': lambda f: f is None }})
    r"""The kind of item this is. For buckets, this is always storage#bucket."""  
    labels: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('labels'), 'exclude': lambda f: f is None }})
    r"""User-provided labels, in key/value pairs."""  
    lifecycle: Optional[BucketLifecycle] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lifecycle'), 'exclude': lambda f: f is None }})
    r"""The bucket's lifecycle configuration. See lifecycle management for more information."""  
    location: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('location'), 'exclude': lambda f: f is None }})
    r"""The location of the bucket. Object data for objects in the bucket resides in physical storage within this region. Defaults to US. See the developer's guide for the authoritative list."""  
    location_type: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('locationType'), 'exclude': lambda f: f is None }})
    r"""The type of the bucket location."""  
    logging: Optional[BucketLogging] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logging'), 'exclude': lambda f: f is None }})
    r"""The bucket's logging configuration, which defines the destination bucket and optional name prefix for the current bucket's logs."""  
    metageneration: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('metageneration'), 'exclude': lambda f: f is None }})
    r"""The metadata generation of this bucket."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""The name of the bucket."""  
    owner: Optional[BucketOwner] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('owner'), 'exclude': lambda f: f is None }})
    r"""The owner of the bucket. This is always the project team's owner group."""  
    project_number: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('projectNumber'), 'exclude': lambda f: f is None }})
    r"""The project number of the project the bucket belongs to."""  
    retention_policy: Optional[BucketRetentionPolicy] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('retentionPolicy'), 'exclude': lambda f: f is None }})
    r"""The bucket's retention policy. The retention policy enforces a minimum retention time for all objects contained in the bucket, based on their creation time. Any attempt to overwrite or delete objects younger than the retention period will result in a PERMISSION_DENIED error. An unlocked retention policy can be modified or removed from the bucket via a storage.buckets.update operation. A locked retention policy cannot be removed or shortened in duration for the lifetime of the bucket. Attempting to remove or decrease period of a locked retention policy will result in a PERMISSION_DENIED error."""  
    rpo: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('rpo'), 'exclude': lambda f: f is None }})
    r"""The Recovery Point Objective (RPO) of this bucket. Set to ASYNC_TURBO to turn on Turbo Replication on a bucket."""  
    satisfies_pzs: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('satisfiesPZS'), 'exclude': lambda f: f is None }})
    r"""Reserved for future use."""  
    self_link: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('selfLink'), 'exclude': lambda f: f is None }})
    r"""The URI of this bucket."""  
    storage_class: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('storageClass'), 'exclude': lambda f: f is None }})
    r"""The bucket's default storage class, used whenever no storageClass is specified for a newly-created object. This defines how objects in the bucket are stored and determines the SLA and the cost of storage. Values include MULTI_REGIONAL, REGIONAL, STANDARD, NEARLINE, COLDLINE, ARCHIVE, and DURABLE_REDUCED_AVAILABILITY. If this value is not specified when the bucket is created, it will default to STANDARD. For more information, see storage classes."""  
    time_created: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('timeCreated'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""The creation time of the bucket in RFC 3339 format."""  
    updated: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('updated'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""The modification time of the bucket in RFC 3339 format."""  
    versioning: Optional[BucketVersioning] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('versioning'), 'exclude': lambda f: f is None }})
    r"""The bucket's versioning configuration."""  
    website: Optional[BucketWebsite] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('website'), 'exclude': lambda f: f is None }})
    r"""The bucket's website configuration, controlling how the service behaves when accessing bucket contents as a web site. See the Static Website Examples for more information."""  
    