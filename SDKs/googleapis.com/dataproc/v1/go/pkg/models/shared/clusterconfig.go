// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

// ClusterConfigInput - The cluster config.
type ClusterConfigInput struct {
	// Autoscaling Policy config associated with the cluster.
	AutoscalingConfig *AutoscalingConfig `json:"autoscalingConfig,omitempty"`
	// Optional. The node group settings.
	AuxiliaryNodeGroups []AuxiliaryNodeGroupInput `json:"auxiliaryNodeGroups,omitempty"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket *string `json:"configBucket,omitempty"`
	// Dataproc metric config.
	DataprocMetricConfig *DataprocMetricConfig `json:"dataprocMetricConfig,omitempty"`
	// Encryption settings for the cluster.
	EncryptionConfig *EncryptionConfig `json:"encryptionConfig,omitempty"`
	// Endpoint config for this cluster
	EndpointConfig *EndpointConfigInput `json:"endpointConfig,omitempty"`
	// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
	GceClusterConfig *GceClusterConfig `json:"gceClusterConfig,omitempty"`
	// The cluster's GKE config.
	GkeClusterConfig *GkeClusterConfig `json:"gkeClusterConfig,omitempty"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions []NodeInitializationAction `json:"initializationActions,omitempty"`
	// Specifies the cluster auto-delete schedule configuration.
	LifecycleConfig *LifecycleConfigInput `json:"lifecycleConfig,omitempty"`
	// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
	MasterConfig *InstanceGroupConfigInput `json:"masterConfig,omitempty"`
	// Specifies a Metastore configuration.
	MetastoreConfig *MetastoreConfig `json:"metastoreConfig,omitempty"`
	// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
	SecondaryWorkerConfig *InstanceGroupConfigInput `json:"secondaryWorkerConfig,omitempty"`
	// Security related configuration, including encryption, Kerberos, etc.
	SecurityConfig *SecurityConfig `json:"securityConfig,omitempty"`
	// Specifies the selection and config of software inside the cluster.
	SoftwareConfig *SoftwareConfig `json:"softwareConfig,omitempty"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket *string `json:"tempBucket,omitempty"`
	// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
	WorkerConfig *InstanceGroupConfigInput `json:"workerConfig,omitempty"`
}

// ClusterConfig - The cluster config.
type ClusterConfig struct {
	// Autoscaling Policy config associated with the cluster.
	AutoscalingConfig *AutoscalingConfig `json:"autoscalingConfig,omitempty"`
	// Optional. The node group settings.
	AuxiliaryNodeGroups []AuxiliaryNodeGroup `json:"auxiliaryNodeGroups,omitempty"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket *string `json:"configBucket,omitempty"`
	// Dataproc metric config.
	DataprocMetricConfig *DataprocMetricConfig `json:"dataprocMetricConfig,omitempty"`
	// Encryption settings for the cluster.
	EncryptionConfig *EncryptionConfig `json:"encryptionConfig,omitempty"`
	// Endpoint config for this cluster
	EndpointConfig *EndpointConfig `json:"endpointConfig,omitempty"`
	// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
	GceClusterConfig *GceClusterConfig `json:"gceClusterConfig,omitempty"`
	// The cluster's GKE config.
	GkeClusterConfig *GkeClusterConfig `json:"gkeClusterConfig,omitempty"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions []NodeInitializationAction `json:"initializationActions,omitempty"`
	// Specifies the cluster auto-delete schedule configuration.
	LifecycleConfig *LifecycleConfig `json:"lifecycleConfig,omitempty"`
	// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
	MasterConfig *InstanceGroupConfig `json:"masterConfig,omitempty"`
	// Specifies a Metastore configuration.
	MetastoreConfig *MetastoreConfig `json:"metastoreConfig,omitempty"`
	// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
	SecondaryWorkerConfig *InstanceGroupConfig `json:"secondaryWorkerConfig,omitempty"`
	// Security related configuration, including encryption, Kerberos, etc.
	SecurityConfig *SecurityConfig `json:"securityConfig,omitempty"`
	// Specifies the selection and config of software inside the cluster.
	SoftwareConfig *SoftwareConfig `json:"softwareConfig,omitempty"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket *string `json:"tempBucket,omitempty"`
	// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
	WorkerConfig *InstanceGroupConfig `json:"workerConfig,omitempty"`
}
