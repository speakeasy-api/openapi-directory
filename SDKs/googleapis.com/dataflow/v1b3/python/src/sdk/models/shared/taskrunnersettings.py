"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import workersettings as shared_workersettings
from dataclasses_json import Undefined, dataclass_json
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class TaskRunnerSettings:
    r"""Taskrunner configuration settings."""
    
    alsologtostderr: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('alsologtostderr'), 'exclude': lambda f: f is None }})
    r"""Whether to also send taskrunner log info to stderr."""  
    base_task_dir: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('baseTaskDir'), 'exclude': lambda f: f is None }})
    r"""The location on the worker for task-specific subdirectories."""  
    base_url: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('baseUrl'), 'exclude': lambda f: f is None }})
    r"""The base URL for the taskrunner to use when accessing Google Cloud APIs. When workers access Google Cloud APIs, they logically do so via relative URLs. If this field is specified, it supplies the base URL to use for resolving these relative URLs. The normative algorithm used is defined by RFC 1808, \\"Relative Uniform Resource Locators\\". If not specified, the default value is \\"http://www.googleapis.com/\\" """  
    commandlines_file_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('commandlinesFileName'), 'exclude': lambda f: f is None }})
    r"""The file to store preprocessing commands in."""  
    continue_on_exception: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('continueOnException'), 'exclude': lambda f: f is None }})
    r"""Whether to continue taskrunner if an exception is hit."""  
    dataflow_api_version: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dataflowApiVersion'), 'exclude': lambda f: f is None }})
    r"""The API version of endpoint, e.g. \\"v1b3\\" """  
    harness_command: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('harnessCommand'), 'exclude': lambda f: f is None }})
    r"""The command to launch the worker harness."""  
    language_hint: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('languageHint'), 'exclude': lambda f: f is None }})
    r"""The suggested backend language."""  
    log_dir: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logDir'), 'exclude': lambda f: f is None }})
    r"""The directory on the VM to store logs."""  
    log_to_serialconsole: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logToSerialconsole'), 'exclude': lambda f: f is None }})
    r"""Whether to send taskrunner log info to Google Compute Engine VM serial console."""  
    log_upload_location: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('logUploadLocation'), 'exclude': lambda f: f is None }})
    r"""Indicates where to put logs. If this is not specified, the logs will not be uploaded. The supported resource type is: Google Cloud Storage: storage.googleapis.com/{bucket}/{object} bucket.storage.googleapis.com/{object}"""  
    oauth_scopes: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('oauthScopes'), 'exclude': lambda f: f is None }})
    r"""The OAuth2 scopes to be requested by the taskrunner in order to access the Cloud Dataflow API."""  
    parallel_worker_settings: Optional[shared_workersettings.WorkerSettings] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('parallelWorkerSettings'), 'exclude': lambda f: f is None }})
    r"""Provides data to pass through to the worker harness."""  
    streaming_worker_main_class: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('streamingWorkerMainClass'), 'exclude': lambda f: f is None }})
    r"""The streaming worker main class name."""  
    task_group: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('taskGroup'), 'exclude': lambda f: f is None }})
    r"""The UNIX group ID on the worker VM to use for tasks launched by taskrunner; e.g. \\"wheel\\"."""  
    task_user: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('taskUser'), 'exclude': lambda f: f is None }})
    r"""The UNIX user ID on the worker VM to use for tasks launched by taskrunner; e.g. \\"root\\"."""  
    temp_storage_prefix: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('tempStoragePrefix'), 'exclude': lambda f: f is None }})
    r"""The prefix of the resources the taskrunner should use for temporary storage. The supported resource type is: Google Cloud Storage: storage.googleapis.com/{bucket}/{object} bucket.storage.googleapis.com/{object}"""  
    vm_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('vmId'), 'exclude': lambda f: f is None }})
    r"""The ID string of the VM."""  
    workflow_file_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('workflowFileName'), 'exclude': lambda f: f is None }})
    r"""The file to store the workflow in."""  
    