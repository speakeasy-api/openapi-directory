<?php

/**
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

declare(strict_types=1);

namespace OpenAPI\OpenAPI\Models\Shared;


/**
 * AsyncInferenceConfig - Specifies configuration for how an endpoint performs asynchronous inference.
 * 
 * @package OpenAPI\OpenAPI\Models\Shared
 * @access public
 */
class AsyncInferenceConfig
{
	#[\JMS\Serializer\Annotation\SerializedName('ClientConfig')]
    #[\JMS\Serializer\Annotation\Type('OpenAPI\OpenAPI\Models\Shared\AsyncInferenceClientConfig')]
    #[\JMS\Serializer\Annotation\SkipWhenEmpty]
    public ?AsyncInferenceClientConfig $clientConfig = null;
    
	#[\JMS\Serializer\Annotation\SerializedName('OutputConfig')]
    #[\JMS\Serializer\Annotation\Type('OpenAPI\OpenAPI\Models\Shared\AsyncInferenceOutputConfig')]
    public AsyncInferenceOutputConfig $outputConfig;
    
	public function __construct()
	{
		$this->clientConfig = null;
		$this->outputConfig = new \OpenAPI\OpenAPI\Models\Shared\AsyncInferenceOutputConfig();
	}
}
