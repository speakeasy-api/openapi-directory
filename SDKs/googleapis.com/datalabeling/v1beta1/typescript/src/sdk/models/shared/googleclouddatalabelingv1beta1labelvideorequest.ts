/*
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

import { SpeakeasyBase, SpeakeasyMetadata } from "../../../internal/utils";
import { GoogleCloudDatalabelingV1beta1EventConfig } from "./googleclouddatalabelingv1beta1eventconfig";
import { GoogleCloudDatalabelingV1beta1HumanAnnotationConfig } from "./googleclouddatalabelingv1beta1humanannotationconfig";
import { GoogleCloudDatalabelingV1beta1ObjectDetectionConfig } from "./googleclouddatalabelingv1beta1objectdetectionconfig";
import { GoogleCloudDatalabelingV1beta1ObjectTrackingConfig } from "./googleclouddatalabelingv1beta1objecttrackingconfig";
import { GoogleCloudDatalabelingV1beta1VideoClassificationConfig } from "./googleclouddatalabelingv1beta1videoclassificationconfig";
import { Expose, Type } from "class-transformer";

/**
 * Required. The type of video labeling task.
 */
export enum GoogleCloudDatalabelingV1beta1LabelVideoRequestFeatureEnum {
  FeatureUnspecified = "FEATURE_UNSPECIFIED",
  Classification = "CLASSIFICATION",
  ObjectDetection = "OBJECT_DETECTION",
  ObjectTracking = "OBJECT_TRACKING",
  Event = "EVENT",
}

/**
 * Request message for LabelVideo.
 */
export class GoogleCloudDatalabelingV1beta1LabelVideoRequest extends SpeakeasyBase {
  /**
   * Configuration for how human labeling task should be done.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "basicConfig" })
  @Type(() => GoogleCloudDatalabelingV1beta1HumanAnnotationConfig)
  basicConfig?: GoogleCloudDatalabelingV1beta1HumanAnnotationConfig;

  /**
   * Config for video event human labeling task.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "eventConfig" })
  @Type(() => GoogleCloudDatalabelingV1beta1EventConfig)
  eventConfig?: GoogleCloudDatalabelingV1beta1EventConfig;

  /**
   * Required. The type of video labeling task.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "feature" })
  feature?: GoogleCloudDatalabelingV1beta1LabelVideoRequestFeatureEnum;

  /**
   * Config for video object detection human labeling task. Object detection will be conducted on the images extracted from the video, and those objects will be labeled with bounding boxes. User need to specify the number of images to be extracted per second as the extraction frame rate.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "objectDetectionConfig" })
  @Type(() => GoogleCloudDatalabelingV1beta1ObjectDetectionConfig)
  objectDetectionConfig?: GoogleCloudDatalabelingV1beta1ObjectDetectionConfig;

  /**
   * Config for video object tracking human labeling task.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "objectTrackingConfig" })
  @Type(() => GoogleCloudDatalabelingV1beta1ObjectTrackingConfig)
  objectTrackingConfig?: GoogleCloudDatalabelingV1beta1ObjectTrackingConfig;

  /**
   * Config for video classification human labeling task. Currently two types of video classification are supported: 1. Assign labels on the entire video. 2. Split the video into multiple video clips based on camera shot, and assign labels on each video clip.
   */
  @SpeakeasyMetadata()
  @Expose({ name: "videoClassificationConfig" })
  @Type(() => GoogleCloudDatalabelingV1beta1VideoClassificationConfig)
  videoClassificationConfig?: GoogleCloudDatalabelingV1beta1VideoClassificationConfig;
}
