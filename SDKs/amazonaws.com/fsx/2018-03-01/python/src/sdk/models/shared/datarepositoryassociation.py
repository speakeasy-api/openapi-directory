"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import dateutil.parser
from ..shared import datarepositoryfailuredetails as shared_datarepositoryfailuredetails
from ..shared import datarepositorylifecycle_enum as shared_datarepositorylifecycle_enum
from ..shared import nfsdatarepositoryconfiguration as shared_nfsdatarepositoryconfiguration
from ..shared import s3datarepositoryconfiguration as shared_s3datarepositoryconfiguration
from ..shared import tag as shared_tag
from dataclasses_json import Undefined, dataclass_json
from datetime import datetime
from marshmallow import fields
from sdk import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DataRepositoryAssociation:
    r"""<p>The configuration of a data repository association that links an Amazon FSx for Lustre file system to an Amazon S3 bucket or an Amazon File Cache resource to an Amazon S3 bucket or an NFS file system. The data repository association configuration object is returned in the response of the following operations:</p> <ul> <li> <p> <code>CreateDataRepositoryAssociation</code> </p> </li> <li> <p> <code>UpdateDataRepositoryAssociation</code> </p> </li> <li> <p> <code>DescribeDataRepositoryAssociations</code> </p> </li> </ul> <p>Data repository associations are supported only for an Amazon FSx for Lustre file system with the <code>Persistent_2</code> deployment type and for an Amazon File Cache resource.</p>"""
    
    association_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('AssociationId'), 'exclude': lambda f: f is None }})  
    batch_import_meta_data_on_create: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('BatchImportMetaDataOnCreate'), 'exclude': lambda f: f is None }})  
    creation_time: Optional[datetime] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('CreationTime'), 'encoder': utils.datetimeisoformat(True), 'decoder': dateutil.parser.isoparse, 'mm_field': fields.DateTime(format='iso'), 'exclude': lambda f: f is None }})
    r"""The time that the resource was created, in seconds (since 1970-01-01T00:00:00Z), also known as Unix time."""  
    data_repository_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DataRepositoryPath'), 'exclude': lambda f: f is None }})  
    data_repository_subdirectories: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('DataRepositorySubdirectories'), 'exclude': lambda f: f is None }})  
    failure_details: Optional[shared_datarepositoryfailuredetails.DataRepositoryFailureDetails] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FailureDetails'), 'exclude': lambda f: f is None }})
    r"""Provides detailed information about the data repository if its <code>Lifecycle</code> is set to <code>MISCONFIGURED</code> or <code>FAILED</code>."""  
    file_cache_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FileCacheId'), 'exclude': lambda f: f is None }})  
    file_cache_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FileCachePath'), 'exclude': lambda f: f is None }})  
    file_system_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FileSystemId'), 'exclude': lambda f: f is None }})
    r"""The globally unique ID of the file system, assigned by Amazon FSx."""  
    file_system_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('FileSystemPath'), 'exclude': lambda f: f is None }})  
    imported_file_chunk_size: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ImportedFileChunkSize'), 'exclude': lambda f: f is None }})  
    lifecycle: Optional[shared_datarepositorylifecycle_enum.DataRepositoryLifecycleEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Lifecycle'), 'exclude': lambda f: f is None }})  
    nfs: Optional[shared_nfsdatarepositoryconfiguration.NFSDataRepositoryConfiguration] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('NFS'), 'exclude': lambda f: f is None }})  
    resource_arn: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('ResourceARN'), 'exclude': lambda f: f is None }})
    r"""The Amazon Resource Name (ARN) for a given resource. ARNs uniquely identify Amazon Web Services resources. We require an ARN when you need to specify a resource unambiguously across all of Amazon Web Services. For more information, see <a href=\\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\\">Amazon Resource Names (ARNs)</a> in the <i>Amazon Web Services General Reference</i>."""  
    s3: Optional[shared_s3datarepositoryconfiguration.S3DataRepositoryConfiguration] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('S3'), 'exclude': lambda f: f is None }})  
    tags: Optional[list[shared_tag.Tag]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('Tags'), 'exclude': lambda f: f is None }})
    r"""A list of <code>Tag</code> values, with a maximum of 50 elements."""  
    