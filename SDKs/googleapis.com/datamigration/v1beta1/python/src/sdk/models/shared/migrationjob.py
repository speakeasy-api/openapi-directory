"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from ..shared import databasetype as shared_databasetype
from ..shared import reversesshconnectivity as shared_reversesshconnectivity
from ..shared import status as shared_status
from ..shared import vpcpeeringconnectivity as shared_vpcpeeringconnectivity
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from sdk import utils
from typing import Any, Optional

class MigrationJobStateEnum(str, Enum):
    r"""The current migration job state."""
    STATE_UNSPECIFIED = 'STATE_UNSPECIFIED'
    MAINTENANCE = 'MAINTENANCE'
    DRAFT = 'DRAFT'
    CREATING = 'CREATING'
    NOT_STARTED = 'NOT_STARTED'
    RUNNING = 'RUNNING'
    FAILED = 'FAILED'
    COMPLETED = 'COMPLETED'
    DELETING = 'DELETING'
    STOPPING = 'STOPPING'
    STOPPED = 'STOPPED'
    DELETED = 'DELETED'
    UPDATING = 'UPDATING'
    STARTING = 'STARTING'
    RESTARTING = 'RESTARTING'
    RESUMING = 'RESUMING'

class MigrationJobTypeEnum(str, Enum):
    r"""Required. The migration job type."""
    TYPE_UNSPECIFIED = 'TYPE_UNSPECIFIED'
    ONE_TIME = 'ONE_TIME'
    CONTINUOUS = 'CONTINUOUS'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class MigrationJobInput:
    r"""Represents a Database Migration Service migration job object."""
    
    destination: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('destination'), 'exclude': lambda f: f is None }})
    r"""Required. The resource name (URI) of the destination connection profile."""  
    destination_database: Optional[shared_databasetype.DatabaseType] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('destinationDatabase'), 'exclude': lambda f: f is None }})
    r"""A message defining the database engine and provider."""  
    display_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('displayName'), 'exclude': lambda f: f is None }})
    r"""The migration job display name."""  
    dump_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dumpPath'), 'exclude': lambda f: f is None }})
    r"""The path to the dump file in Google Cloud Storage, in the format: (gs://[BUCKET_NAME]/[OBJECT_NAME])."""  
    error: Optional[shared_status.Status] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('error'), 'exclude': lambda f: f is None }})
    r"""The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors)."""  
    labels: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('labels'), 'exclude': lambda f: f is None }})
    r"""The resource labels for migration job to use to annotate any related underlying resources such as Compute Engine VMs. An object containing a list of \\"key\\": \\"value\\" pairs. Example: `{ \\"name\\": \\"wrench\\", \\"mass\\": \\"1.3kg\\", \\"count\\": \\"3\\" }`."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""The name (URI) of this migration job resource, in the form of: projects/{project}/locations/{location}/migrationJobs/{migrationJob}."""  
    reverse_ssh_connectivity: Optional[shared_reversesshconnectivity.ReverseSSHConnectivity] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('reverseSshConnectivity'), 'exclude': lambda f: f is None }})
    r"""The details needed to configure a reverse SSH tunnel between the source and destination databases. These details will be used when calling the generateSshScript method (see https://cloud.google.com/database-migration/docs/reference/rest/v1beta1/projects.locations.migrationJobs/generateSshScript) to produce the script that will help set up the reverse SSH tunnel, and to set up the VPC peering between the Cloud SQL private network and the VPC."""  
    source: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('source'), 'exclude': lambda f: f is None }})
    r"""Required. The resource name (URI) of the source connection profile."""  
    source_database: Optional[shared_databasetype.DatabaseType] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sourceDatabase'), 'exclude': lambda f: f is None }})
    r"""A message defining the database engine and provider."""  
    state: Optional[MigrationJobStateEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('state'), 'exclude': lambda f: f is None }})
    r"""The current migration job state."""  
    static_ip_connectivity: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('staticIpConnectivity'), 'exclude': lambda f: f is None }})
    r"""The source database will allow incoming connections from the destination database's public IP. You can retrieve the Cloud SQL instance's public IP from the Cloud SQL console or using Cloud SQL APIs. No additional configuration is required."""  
    type: Optional[MigrationJobTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""Required. The migration job type."""  
    vpc_peering_connectivity: Optional[shared_vpcpeeringconnectivity.VpcPeeringConnectivity] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('vpcPeeringConnectivity'), 'exclude': lambda f: f is None }})
    r"""The details of the VPC where the source database is located in Google Cloud. We will use this information to set up the VPC peering connection between Cloud SQL and this VPC."""  
    
class MigrationJobPhaseEnum(str, Enum):
    r"""Output only. The current migration job phase."""
    PHASE_UNSPECIFIED = 'PHASE_UNSPECIFIED'
    FULL_DUMP = 'FULL_DUMP'
    CDC = 'CDC'
    PROMOTE_IN_PROGRESS = 'PROMOTE_IN_PROGRESS'
    WAITING_FOR_SOURCE_WRITES_TO_STOP = 'WAITING_FOR_SOURCE_WRITES_TO_STOP'
    PREPARING_THE_DUMP = 'PREPARING_THE_DUMP'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class MigrationJob:
    r"""Represents a Database Migration Service migration job object."""
    
    create_time: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('createTime'), 'exclude': lambda f: f is None }})
    r"""Output only. The timestamp when the migration job resource was created. A timestamp in RFC3339 UTC \\"Zulu\\" format, accurate to nanoseconds. Example: \\"2014-10-02T15:01:23.045123456Z\\"."""  
    destination: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('destination'), 'exclude': lambda f: f is None }})
    r"""Required. The resource name (URI) of the destination connection profile."""  
    destination_database: Optional[shared_databasetype.DatabaseType] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('destinationDatabase'), 'exclude': lambda f: f is None }})
    r"""A message defining the database engine and provider."""  
    display_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('displayName'), 'exclude': lambda f: f is None }})
    r"""The migration job display name."""  
    dump_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dumpPath'), 'exclude': lambda f: f is None }})
    r"""The path to the dump file in Google Cloud Storage, in the format: (gs://[BUCKET_NAME]/[OBJECT_NAME])."""  
    duration: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('duration'), 'exclude': lambda f: f is None }})
    r"""Output only. The duration of the migration job (in seconds). A duration in seconds with up to nine fractional digits, terminated by 's'. Example: \\"3.5s\\"."""  
    end_time: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('endTime'), 'exclude': lambda f: f is None }})
    r"""Output only. If the migration job is completed, the time when it was completed."""  
    error: Optional[shared_status.Status] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('error'), 'exclude': lambda f: f is None }})
    r"""The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors)."""  
    labels: Optional[dict[str, str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('labels'), 'exclude': lambda f: f is None }})
    r"""The resource labels for migration job to use to annotate any related underlying resources such as Compute Engine VMs. An object containing a list of \\"key\\": \\"value\\" pairs. Example: `{ \\"name\\": \\"wrench\\", \\"mass\\": \\"1.3kg\\", \\"count\\": \\"3\\" }`."""  
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""The name (URI) of this migration job resource, in the form of: projects/{project}/locations/{location}/migrationJobs/{migrationJob}."""  
    phase: Optional[MigrationJobPhaseEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('phase'), 'exclude': lambda f: f is None }})
    r"""Output only. The current migration job phase."""  
    reverse_ssh_connectivity: Optional[shared_reversesshconnectivity.ReverseSSHConnectivity] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('reverseSshConnectivity'), 'exclude': lambda f: f is None }})
    r"""The details needed to configure a reverse SSH tunnel between the source and destination databases. These details will be used when calling the generateSshScript method (see https://cloud.google.com/database-migration/docs/reference/rest/v1beta1/projects.locations.migrationJobs/generateSshScript) to produce the script that will help set up the reverse SSH tunnel, and to set up the VPC peering between the Cloud SQL private network and the VPC."""  
    source: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('source'), 'exclude': lambda f: f is None }})
    r"""Required. The resource name (URI) of the source connection profile."""  
    source_database: Optional[shared_databasetype.DatabaseType] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sourceDatabase'), 'exclude': lambda f: f is None }})
    r"""A message defining the database engine and provider."""  
    state: Optional[MigrationJobStateEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('state'), 'exclude': lambda f: f is None }})
    r"""The current migration job state."""  
    static_ip_connectivity: Optional[dict[str, Any]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('staticIpConnectivity'), 'exclude': lambda f: f is None }})
    r"""The source database will allow incoming connections from the destination database's public IP. You can retrieve the Cloud SQL instance's public IP from the Cloud SQL console or using Cloud SQL APIs. No additional configuration is required."""  
    type: Optional[MigrationJobTypeEnum] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""Required. The migration job type."""  
    update_time: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('updateTime'), 'exclude': lambda f: f is None }})
    r"""Output only. The timestamp when the migration job resource was last updated. A timestamp in RFC3339 UTC \\"Zulu\\" format, accurate to nanoseconds. Example: \\"2014-10-02T15:01:23.045123456Z\\"."""  
    vpc_peering_connectivity: Optional[shared_vpcpeeringconnectivity.VpcPeeringConnectivity] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('vpcPeeringConnectivity'), 'exclude': lambda f: f is None }})
    r"""The details of the VPC where the source database is located in Google Cloud. We will use this information to set up the VPC peering connection between Cloud SQL and this VPC."""  
    